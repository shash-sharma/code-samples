{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pruning_units.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "tYqrMVdpA1NX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision as tv\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BhzbK8ntAmnN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Computation Routines"
      ]
    },
    {
      "metadata": {
        "id": "NuhlcxSIA94G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ]
    },
    {
      "metadata": {
        "id": "YC8SpzkJA8gK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DatasetManager:\n",
        "    \n",
        "    def __init__(self, dataset = 'cifar10', percent_data = 10.0, percent_val = 20.0, data_path = './data'):\n",
        "        \n",
        "        # 'dataset' can be 'hymenoptera', 'cifar10', or 'cifar100'.\n",
        "        # 'percent_data' is the percentage of the full training set to be used.\n",
        "        # 'percent_val' is the percentage of the *loaded* training set to be used as validation data.\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.data_path = data_path\n",
        "        self.percent_data = percent_data\n",
        "        self.percent_val = percent_val\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "            \n",
        "        elif self.dataset == 'cifar10' or self.dataset == 'cifar100':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def ImportDataset(self, batch_size=5):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "        \n",
        "            self.trainset = tv.datasets.ImageFolder(root=self.data_path,\n",
        "                             transform=self.transform)\n",
        "        \n",
        "        # todo\n",
        "        \n",
        "        elif self.dataset == 'cifar10':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR10(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR10(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "        \n",
        "        elif self.dataset == 'cifar100':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR100(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR100(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "             \n",
        "        self.SplitData();\n",
        "        self.GenerateLoaders();\n",
        "                \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def SplitData(self):\n",
        "        \n",
        "        len_full = self.trainset.__len__()\n",
        "        len_train = int(np.round(len_full*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.trainset = torch.utils.data.random_split(self.trainset, (len_full-len_train, len_train))\n",
        "        \n",
        "        len_val = int(np.round(len_train*self.percent_val/100.0))\n",
        "        len_train = len_train - len_val\n",
        "        \n",
        "        self.valset, self.trainset = torch.utils.data.random_split(self.trainset, (len_val, len_train))\n",
        "         \n",
        "        len_full_test = self.testset.__len__()\n",
        "        len_test = int(np.round(len_full_test*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.testset = torch.utils.data.random_split(self.testset, (len_full_test-len_test, len_test))\n",
        "\n",
        "        print('\\nFull training set size: {}'.format(len_full))\n",
        "        print('Full test set size: {}'.format(len_full_test))\n",
        "        print('\\nActive training set size: {}'.format(len_train))\n",
        "        print('Active validation set size: {}'.format(len_val))\n",
        "        print('Active test set size: {}\\n'.format(len_test))\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def GenerateLoaders(self):\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.val_loader = torch.utils.data.DataLoader(self.valset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)          \n",
        "            \n",
        "        return\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUJWD_oDBKHD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training function"
      ]
    },
    {
      "metadata": {
        "id": "34G6XlaFBMUW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, dat, criterion, optimizer, scheduler, prune_settings=0, num_epochs=25):\n",
        "    \n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                dataloader = dat.train_loader\n",
        "                dataset_size = dat.trainset.__len__()\n",
        "                \n",
        "                model.train()  # Set model to training mode\n",
        "                \n",
        "            else:\n",
        "                \n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                dataloader = dat.val_loader\n",
        "                dataset_size = dat.valset.__len__()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloader:\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                if prune_settings != 0:\n",
        "                    TrackConv2DNorms(model, prune_settings, inputs)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if training\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            # Record losses for later use, plotting etc\n",
        "            if phase == 'train':\n",
        "                prune_settings.epoch_loss.append(epoch_loss)\n",
        "                prune_settings.epoch_acc.append(epoch_acc)\n",
        "            elif phase == 'val':\n",
        "                prune_settings.val_loss.append(epoch_loss)\n",
        "                prune_settings.val_acc.append(epoch_acc)\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z7C3UubBBXFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruning functions"
      ]
    },
    {
      "metadata": {
        "id": "MaSebsFceMRD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning settings"
      ]
    },
    {
      "metadata": {
        "id": "jAZVF5O-BZNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Constants that define possible pruning metrics\n",
        "WEIGHT_NORM = 1\n",
        "ACT_NORM = 2\n",
        "\n",
        "# Class that contains various settings pertaining to how filters are pruned\n",
        "class UnitPruningSettings:\n",
        "    \n",
        "    def __init__(self, idx_layer=0, idx_filter=0, N_prune=1, P_prune=10, p=2, pruning_metric=WEIGHT_NORM):\n",
        "        \n",
        "        # EITHER N_prune OR P_prune will be used to decide how many filters to prune.\n",
        "        # If one is non-positive, the other is used.\n",
        "        # If neither is non-positive, priority is given to P_prune.\n",
        "        # If both are non-positive, no pruning will happen.\n",
        "\n",
        "        self.N_prune = N_prune # Number of filters allowed to be pruned in one pass\n",
        "        self.P_prune = P_prune; # Percent of filters of the current layer to prune\n",
        "        \n",
        "        self.idx_filter = idx_filter # Indices of the N_prune filters\n",
        "        self.idx_layer = idx_layer # Current layer under consideration\n",
        "        self.p = p # p-norm to use when computing which filters to remove\n",
        "        self.pruning_metric = pruning_metric\n",
        "        \n",
        "        self.norms_botk = []\n",
        "        self.idx_norms_botk = []\n",
        "        \n",
        "        # Various statistics will be stored and computed to keep track of how the network changes\n",
        "        \n",
        "        # Number of filters per layer in the original network\n",
        "        self.filters_per_layer_orig = []\n",
        "        \n",
        "        # Number of filters per layer after pruning - this gets updated every time the network is pruned\n",
        "        self.filters_per_layer_after = []\n",
        "        \n",
        "        # Time taken to prune in sec (running total, updated every time pruning happens)\n",
        "        self.prune_time = 0.0\n",
        "        \n",
        "        # Keep track of running epoch loss and validation loss, and corresponding accuracy\n",
        "        self.epoch_loss = []\n",
        "        self.val_loss = []\n",
        "        self.epoch_acc = []\n",
        "        self.val_acc = []\n",
        "        \n",
        "        return\n",
        "    \n",
        "    # Function to print the current pruning state of the model. Verbose can be 0, 1, or 2.\n",
        "    def PrintPruningStatistics(self, verbose=1):\n",
        "    \n",
        "        if verbose == 0:\n",
        "            return\n",
        "        \n",
        "        print(\"Total number of filters before pruning: {}\".format(sum(self.filters_per_layer_orig)))\n",
        "        print(\"Total number of filters after pruning: {}\".format(sum(self.filters_per_layer_after)))\n",
        "    \n",
        "        return\n",
        "    \n",
        "    # Function to set up and initialize based on a given model\n",
        "    def Setup(self, model):\n",
        "        \n",
        "        # Count the number of conv layers\n",
        "        self.N_layers = 0\n",
        "        \n",
        "        for layer, (name, module) in enumerate(model.features._modules.items()):\n",
        "            self.N_layers += 1\n",
        "                    \n",
        "        # Initialize storage containers\n",
        "        self.norms_botk = [None]*self.N_layers\n",
        "        self.idx_norms_botk = [None]*self.N_layers\n",
        "        \n",
        "    \n",
        "    # Function to reset norm containers\n",
        "    def ResetNormContainers(self):\n",
        "    \n",
        "        self.norms_botk = [None]*self.N_layers\n",
        "        self.idx_norms_botk = [None]*self.N_layers\n",
        "    \n",
        "        return\n",
        "\n",
        "    # Function to reset filter containers\n",
        "    def ResetFilterContainers(self):\n",
        "    \n",
        "        self.filters_per_layer_orig = []\n",
        "        self.filters_per_layer_after = []\n",
        "    \n",
        "        return\n",
        "\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qc41Ng14ezag",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning decisions"
      ]
    },
    {
      "metadata": {
        "id": "2F0n6VUTe2ND",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to compute the p-norm of weights in all filters of a given layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DWeightNorms(model, idx_layer, p):\n",
        "    \n",
        "    # Extract the layer of the model currently being considered\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    weights = conv.weight.data\n",
        "\n",
        "    # Compute norms of each filter\n",
        "    norms = weights.norm(p, dim=2).norm(p, dim=2).norm(p, dim=1)\n",
        "    \n",
        "    return norms\n",
        "\n",
        "\n",
        "# Function to compute the p-norm of activations in all filters per layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DActNorms(activation, prune_settings):\n",
        "    \n",
        "    p = prune_settings.p\n",
        "    \n",
        "    # Compute norms of each activation\n",
        "    norms = torch.norm(activation, p, dim=0).norm(p, dim=1).norm(p, dim=1)\n",
        "        \n",
        "    return norms\n",
        "\n",
        "\n",
        "# Function to track the p-norm of activations of all filters of during training.\n",
        "def TrackConv2DNorms(model, prune_settings, inputs):\n",
        "    \n",
        "    p = prune_settings.p\n",
        "    P_prune = prune_settings.P_prune\n",
        "\n",
        "    x = Variable(inputs)\n",
        "\n",
        "    ii = -1\n",
        "    for layer, (name, module) in enumerate(model.features._modules.items()):\n",
        "        ii += 1\n",
        "        x = module(x)\n",
        "        \n",
        "        if isinstance(module, torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            if prune_settings.pruning_metric == WEIGHT_NORM:\n",
        "                norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "            elif prune_settings.pruning_metric == ACT_NORM:\n",
        "                norms = ComputeConv2DActNorms(x, prune_settings)\n",
        "\n",
        "            # Use the given prune percentage to figure out how many filters to prune\n",
        "            if (P_prune >= 0):\n",
        "                N_prune = int(len(norms.float())*P_prune/100.0)\n",
        "                prune_settings.N_prune = N_prune\n",
        "\n",
        "#             n_botk, ind_botk = torch.topk(norms, N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            norms = norms.cpu().detach().numpy()\n",
        "    \n",
        "            # Store the norms for each filter\n",
        "#             if prune_settings.norms_botk[ii] is None:\n",
        "#                 prune_settings.norms_botk[ii] = norms\n",
        "#             else:\n",
        "#                 prune_settings.norms_botk[ii] += norms\n",
        "                \n",
        "            # Store normalized norms for each filter\n",
        "            if prune_settings.norms_botk[ii] is None:\n",
        "                prune_settings.norms_botk[ii] = norms/max(norms)\n",
        "            else:\n",
        "                prune_settings.norms_botk[ii] += norms/max(norms)\n",
        "                \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U59vXkzfehDX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning workers"
      ]
    },
    {
      "metadata": {
        "id": "RY5GZWxXej1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The following functions were adapted from https://github.com/jacobgil/pytorch-pruning/blob/master/prune.py\n",
        "\n",
        "def replace_layers(model, i, idx, layers):\n",
        "\tif i in idx:\n",
        "\t\treturn layers[idx.index(i)]\n",
        "\treturn model[i]\n",
        "\n",
        "\n",
        "# Function to prune a given convolution layer in the model provided.\n",
        "# Input \"idx_layers\" is the global index of the convolution layer to be pruned.\n",
        "# Input \"prune_settings\" is a data structure containing information on how pruning is performed.\n",
        "def PruneConvLayers(model, prune_settings):\n",
        "    \n",
        "    # Strategy: in order to prune a particular layer, the output of the previous layer \n",
        "    # and the inputs to the next layer must also be altered accordingly.\n",
        "\t\n",
        "    # Extract pruning settings for convenience\n",
        "    N_prune = prune_settings.N_prune\n",
        "    idx_filter = prune_settings.idx_filter\n",
        "    idx_layer = prune_settings.idx_layer\n",
        "    \n",
        "    if idx_layer >= len(model.features._modules.items()):\n",
        "        return\n",
        "        \n",
        "    # Extract the layer of the model currently being pruned\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    \n",
        "\n",
        "    # In case the list of target filters to delete has out-of-range entries, detect and ignore them\n",
        "    del_filters = []\n",
        "    for kk in range(0, len(idx_filter)):\n",
        "        if idx_filter[kk] >= conv.out_channels:\n",
        "            del_filters.extend(kk)\n",
        "    \n",
        "    if (len(del_filters) > 0):\n",
        "        idx_filter = np.delete(idx_filter, del_filters, 0)\n",
        "        N_prune = len(idx_filter)\n",
        "        prune_settings.N_prune = N_prune\n",
        "        print(\"[WARNING] Encountered an out-of-range target filter; it will be ignored.\")\n",
        "    \n",
        "    # Record pruning statistics\n",
        "    prune_settings.filters_per_layer_orig[idx_layer] = conv.out_channels\n",
        "    prune_settings.filters_per_layer_after[idx_layer] = conv.out_channels - N_prune\n",
        "    \n",
        "        \n",
        "    # To keep track of the succeeding convolution layer\n",
        "    next_conv = None\n",
        "    offset = 1\n",
        "    \n",
        "    # Figure out how many layers after this one are NOT conv layers, in order to skip pruning them\n",
        "    while idx_layer + offset < len(model.features._modules.items()):\n",
        "        \n",
        "        res =  list(model.features._modules.items())[idx_layer + offset]\n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            next_name, next_conv = res\n",
        "            break\n",
        "        offset = offset + 1\n",
        "    \n",
        "    # Create a new, replacement conv layer to remove a given number of filters.\n",
        "    # The rest of its settings should remain the same as the original conv layer.\n",
        "    new_conv = torch.nn.Conv2d(in_channels = conv.in_channels,\n",
        "                               out_channels = conv.out_channels - N_prune,\n",
        "\t\t\t                   kernel_size = conv.kernel_size,\n",
        "                               stride = conv.stride,\n",
        "                               padding = conv.padding,\n",
        "                               dilation = conv.dilation,\n",
        "                               groups = conv.groups,\n",
        "                               bias = True)\n",
        "    \n",
        "    new_conv.bias = conv.bias\n",
        "    \n",
        "    # Copy over the weights to the new conv layer, except the ones corresponding to the filter to be removed\n",
        "    old_weights = conv.weight.data.cpu().numpy()\n",
        "    new_weights = new_conv.weight.data.cpu().numpy()\n",
        "    \n",
        "    # Copy over the set of filters, excluding the ones to be removed\n",
        "    new_weights_temp = np.copy(old_weights)\n",
        "    new_weights_temp = np.delete(new_weights_temp, idx_filter, 0)\n",
        "    new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "    # Update weight data of the new conv layer\n",
        "    new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "    \n",
        "    # Now do the same thing for biases\n",
        "    old_biases = conv.bias.data.cpu().numpy()\n",
        "    new_biases = np.zeros(shape=(old_biases.shape[0] - N_prune), dtype=np.float32)\n",
        "    \n",
        "    new_biases_temp = np.copy(old_biases)\n",
        "    new_biases_temp = np.delete(new_biases_temp, idx_filter, 0)\n",
        "    new_biases[:] = new_biases_temp[:]\n",
        "        \n",
        "    new_conv.bias.data = torch.from_numpy(new_biases).cuda()\n",
        "    \n",
        "    # If there is a succeeding conv layer, adjust its input units and weights accordingly\n",
        "    if next_conv != None:\n",
        "        \n",
        "        next_new_conv = torch.nn.Conv2d(in_channels = next_conv.in_channels - N_prune,\n",
        "                                        out_channels =  next_conv.out_channels,\n",
        "                                        kernel_size = next_conv.kernel_size,\n",
        "                                        stride = next_conv.stride,\n",
        "                                        padding = next_conv.padding,\n",
        "                                        dilation = next_conv.dilation,\n",
        "                                        groups = next_conv.groups,\n",
        "                                        bias = True)\n",
        "        \n",
        "        next_new_conv.bias = next_conv.bias\n",
        "\n",
        "        old_weights = next_conv.weight.data.cpu().numpy()\n",
        "        new_weights = next_new_conv.weight.data.cpu().numpy()\n",
        "        \n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_filter, 1)\n",
        "        new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "        next_new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        # Now do the same thing for biases\n",
        "        next_new_conv.bias.data = next_conv.bias.data\n",
        "\n",
        "        # Update the actual model by replacing the existing filters with the new ones\n",
        "        features = torch.nn.Sequential(\n",
        "                *(replace_layers(model.features, i, [idx_layer, idx_layer + offset], \\\n",
        "                    [new_conv, next_new_conv]) for i, _ in enumerate(model.features)))\n",
        "        del model.features\n",
        "        del conv\n",
        "\n",
        "        model.features = features\n",
        "    \n",
        "    else:\n",
        "\n",
        "        # This is the last conv layer. This affects the first linear layer of the classifier.\n",
        "        model.features = torch.nn.Sequential(*(replace_layers(model.features, i, [idx_layer], [new_conv]) for i, _ in enumerate(model.features)))\n",
        "        idx_layer = 0\n",
        "        old_linear_layer = None\n",
        "\n",
        "        for _, module in model.classifier._modules.items():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                old_linear_layer = module\n",
        "                break\n",
        "            idx_layer = idx_layer + 1\n",
        "\n",
        "        if old_linear_layer == None:\n",
        "            raise BaseException(\"No linear layer found in classifier.\")\n",
        "            \n",
        "        params_per_input_channel = int(old_linear_layer.in_features/conv.out_channels)\n",
        "\n",
        "        new_linear_layer = torch.nn.Linear(old_linear_layer.in_features - N_prune*params_per_input_channel, \n",
        "                                           old_linear_layer.out_features)\n",
        "\n",
        "        old_weights = old_linear_layer.weight.data.cpu().numpy()\n",
        "        new_weights = new_linear_layer.weight.data.cpu().numpy()\t \t\n",
        "\n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        idx_expanded = np.zeros(shape=(N_prune*params_per_input_channel))\n",
        "        \n",
        "        for kk in range(0, len(idx_filter)):\n",
        "            idx_expanded[kk*params_per_input_channel:kk*params_per_input_channel+params_per_input_channel] = np.arange(idx_filter[kk]*params_per_input_channel, idx_filter[kk]*params_per_input_channel + params_per_input_channel)\n",
        "\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_expanded.astype(int), 1)\n",
        "        new_weights[:, :] = new_weights_temp[:, :]\n",
        "        \n",
        "        new_linear_layer.bias.data = old_linear_layer.bias.data\n",
        "        new_linear_layer.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        classifier = torch.nn.Sequential(*(replace_layers(model.classifier, i, [idx_layer], [new_linear_layer]) for i, _ in enumerate(model.classifier)))\n",
        "\n",
        "        del model.classifier\n",
        "        del next_conv\n",
        "        del conv\n",
        "        model.classifier = classifier\n",
        "        \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7h5EFGhe-PJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning driver"
      ]
    },
    {
      "metadata": {
        "id": "Xp8V5o_UfADH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to iterate through all conv2D layers of the network and determine \n",
        "# filters to be pruned, and then carry out the pruning.\n",
        "def PruneAllConv2DLayers(model, prune_settings):\n",
        "    \n",
        "    # Extract pruning settings for convenience\n",
        "    # Note that \"N_prune\" *consecutive* filters will get pruned\n",
        "    N_prune = prune_settings.N_prune\n",
        "    P_prune = prune_settings.P_prune\n",
        "    p = prune_settings.p\n",
        "    pruning_metric = prune_settings.pruning_metric\n",
        "    \n",
        "    # Count number of prunable layers for preallocation\n",
        "    N_layers = len(model.features._modules.items())       \n",
        "    prune_settings.filters_per_layer_orig = np.zeros(shape=(1, N_layers)).ravel()\n",
        "    prune_settings.filters_per_layer_after = np.zeros(shape=(1, N_layers)).ravel()\n",
        "\n",
        "    \n",
        "    # Find the N_prune filters to remove\n",
        "    ii = 0\n",
        "    while ii < len(model.features._modules.items()):\n",
        "        \n",
        "        res = list(model.features._modules.items())[ii]\n",
        "        \n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            _, conv = list(model.features._modules.items())[ii]\n",
        "            \n",
        "            # Record pruning statistics\n",
        "            prune_settings.filters_per_layer_orig[ii] = conv.out_channels\n",
        "            prune_settings.filters_per_layer_after[ii] = conv.out_channels\n",
        "            \n",
        "            # Compute values and indices of the N_prune smallest norms\n",
        "#             if pruning_metric == WEIGHT_NORM:\n",
        "#                 norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "#             elif pruning_metric == ACT_NORM:\n",
        "# #                 norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "#                 norms = ComputeConv2DActNorms(res[1], prune_settings)\n",
        "                \n",
        "        \n",
        "            if (P_prune >= 0):\n",
        "                N_prune = int(conv.out_channels*P_prune/100.0)\n",
        "                prune_settings.N_prune = N_prune\n",
        "            \n",
        "            if prune_settings.norms_botk[ii] is not None:\n",
        "                \n",
        "#                 n_botk, ind_botk = torch.topk(torch.from_numpy(prune_settings.norms_botk[ii]), N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            \n",
        "                norms = np.asarray(prune_settings.norms_botk[ii]).ravel()\n",
        "                ind_botk = np.argpartition(norms, N_prune)    \n",
        "                n_botk = norms[ind_botk[:N_prune]]\n",
        "                ind_botk = ind_botk[:N_prune]\n",
        "        \n",
        "                prune_settings.idx_layer = ii\n",
        "                prune_settings.idx_filter = ind_botk\n",
        "\n",
        "                model = PruneConvLayers(model, prune_settings)\n",
        "                \n",
        "        ii = ii + 1\n",
        "            \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cCEASFCwB3uP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Pruning"
      ]
    },
    {
      "metadata": {
        "id": "LmuhhazDB6d7",
        "colab_type": "code",
        "outputId": "21090ddb-ae77-4b19-85af-294d705be9e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# Test pruning\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.train()\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10, 12, 15, 16, 21), \n",
        "                                     N_prune=5, p=2, pruning_metric=WEIGHT_NORM)\n",
        "# prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10), \n",
        "#                                      N_prune=1, p=2, pruning_metric=WEIGHT_NORM)\n",
        "\n",
        "N_layers = len(model.features._modules.items())       \n",
        "prune_settings.filters_per_layer_orig = np.zeros(shape=(1, N_layers)).ravel()\n",
        "prune_settings.filters_per_layer_after = np.zeros(shape=(1, N_layers)).ravel()\n",
        "\n",
        "t0 = time.time()\n",
        "model = PruneConvLayers(model, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.torch/models/vgg16-397923af.pth\n",
            "553433881it [00:11, 49592333.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Pruning took 10.873862981796265 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0tfmvixSCzAH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Setup Routines"
      ]
    },
    {
      "metadata": {
        "id": "fF2ZhlgCC31h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline Model Setup"
      ]
    },
    {
      "metadata": {
        "id": "GeD0hZfpDHbr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_baseline = models.vgg16(pretrained=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_baseline = model_baseline.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model_baseline.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lc0YTkyADA63",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruned Model Setup"
      ]
    },
    {
      "metadata": {
        "id": "cr4wgE6eDKND",
        "colab_type": "code",
        "outputId": "0f2c5d59-5f0f-4a76-918f-858c3ab87e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "cell_type": "code",
      "source": [
        "# Test pruning all layers\n",
        "\n",
        "model_pruned = models.vgg16(pretrained=True)\n",
        "model_pruned.train()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_pruned = model_pruned.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model_pruned.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(28, 10, N_prune = 4, p = 2, pruning_metric = WEIGHT_NORM)\n",
        "\n",
        "t0 = time.time()\n",
        "model_pruned = PruneAllConv2DLayers(model_pruned, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c16a3b839ee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel_pruned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPruneAllConv2DLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pruned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Pruning took {} s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-68787035a1e4>\u001b[0m in \u001b[0;36mPruneAllConv2DLayers\u001b[0;34m(model, prune_settings)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mprune_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_prune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN_prune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mprune_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorms_botk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#                 n_botk, ind_botk = torch.topk(torch.from_numpy(prune_settings.norms_botk[ii]), N_prune, 0, largest=False, sorted=True, out=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "coPfbZmXE3T0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Driver Routines"
      ]
    },
    {
      "metadata": {
        "id": "DS7qbZ0JE6Fs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Iterative Pruning"
      ]
    },
    {
      "metadata": {
        "id": "E1q-h_KTE-kE",
        "colab_type": "code",
        "outputId": "cf5c1359-2e28-4021-888f-a18eafa99a55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11590
        }
      },
      "cell_type": "code",
      "source": [
        "# ====== Dataset setup ======\n",
        "\n",
        "percent_data = 1.0\n",
        "percent_val = 20.0\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "# ====== Model setup ======\n",
        "\n",
        "model = models.vgg16(pretrained=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# ====== Pruning setup ======\n",
        "\n",
        "N_prune = 0\n",
        "P_prune = 5\n",
        "p = 2\n",
        "prune_settings = UnitPruningSettings(N_prune=N_prune, \n",
        "                                     P_prune=P_prune, \n",
        "                                     p=p, \n",
        "                                     pruning_metric=WEIGHT_NORM)\n",
        "prune_settings.Setup(model)\n",
        "\n",
        "\n",
        "# ====== Begin training ======\n",
        "\n",
        "N_iter_outer = 10\n",
        "N_iter_inner = 10\n",
        "\n",
        "# Import data\n",
        "dat = DatasetManager(dataset='cifar10', \n",
        "                     percent_data=percent_data, \n",
        "                     percent_val=percent_val)\n",
        "\n",
        "dat.ImportDataset(batch_size=batch_size)\n",
        "\n",
        "for ii in range(0, N_iter_outer):\n",
        "    \n",
        "    print(\"\\n------ Outer iteration {}/{} ------\".format(ii+1, N_iter_outer))\n",
        "#     t0 = time.time()\n",
        "\n",
        "    # ------ Prune current model ------\n",
        "        \n",
        "    model = PruneAllConv2DLayers(model, prune_settings)\n",
        "    new_model = copy.deepcopy(model)\n",
        "    model = new_model\n",
        "    prune_settings.PrintPruningStatistics(1)\n",
        "    prune_settings.ResetNormContainers()\n",
        "\n",
        "    # ------ Train current model ------\n",
        "    \n",
        "    # Import data\n",
        "    dat = DatasetManager(dataset='cifar10', percent_data=percent_data, percent_val=percent_val)\n",
        "    dat.ImportDataset(batch_size=batch_size)\n",
        "    \n",
        "    # Update optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, prune_settings, num_epochs=N_iter_inner)\n",
        "        \n",
        "#     print (\"Pruning took {} s\".format(time.time() - t0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "\n",
            "------ Outer iteration 1/10 ------\n",
            "Total number of filters before pruning: 4224.0\n",
            "Total number of filters after pruning: 4224.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 4.5017 Acc: 0.0925\n",
            "val Loss: 2.9637 Acc: 0.0900\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 2.6241 Acc: 0.1200\n",
            "val Loss: 2.4863 Acc: 0.0400\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 2.5566 Acc: 0.0925\n",
            "val Loss: 2.5447 Acc: 0.0400\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 2.4681 Acc: 0.1100\n",
            "val Loss: 2.4994 Acc: 0.1800\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 2.3919 Acc: 0.1325\n",
            "val Loss: 2.2881 Acc: 0.0600\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 2.3577 Acc: 0.1575\n",
            "val Loss: 2.2471 Acc: 0.1700\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 2.3858 Acc: 0.1375\n",
            "val Loss: 2.3859 Acc: 0.0400\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 2.2565 Acc: 0.1525\n",
            "val Loss: 2.2581 Acc: 0.1800\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 2.2306 Acc: 0.1650\n",
            "val Loss: 2.1933 Acc: 0.1200\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 2.1727 Acc: 0.1900\n",
            "val Loss: 2.0962 Acc: 0.1600\n",
            "\n",
            "Training complete in 2m 50s\n",
            "Best val Acc: 0.180000\n",
            "\n",
            "------ Outer iteration 2/10 ------\n",
            "Total number of filters before pruning: 4224.0\n",
            "Total number of filters after pruning: 4020.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 2.4235 Acc: 0.1450\n",
            "val Loss: 2.4970 Acc: 0.1400\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 2.4313 Acc: 0.1300\n",
            "val Loss: 2.4998 Acc: 0.1300\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 2.3722 Acc: 0.1400\n",
            "val Loss: 2.3762 Acc: 0.1900\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 2.2630 Acc: 0.1775\n",
            "val Loss: 2.2752 Acc: 0.1600\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 2.2729 Acc: 0.1900\n",
            "val Loss: 2.0703 Acc: 0.2300\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 2.2627 Acc: 0.1875\n",
            "val Loss: 2.1675 Acc: 0.1800\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 2.2619 Acc: 0.1425\n",
            "val Loss: 2.3237 Acc: 0.0800\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 2.1448 Acc: 0.1900\n",
            "val Loss: 2.0400 Acc: 0.2200\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 2.0297 Acc: 0.2475\n",
            "val Loss: 2.0364 Acc: 0.2200\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 2.0926 Acc: 0.2175\n",
            "val Loss: 2.0444 Acc: 0.2600\n",
            "\n",
            "Training complete in 2m 45s\n",
            "Best val Acc: 0.260000\n",
            "\n",
            "------ Outer iteration 3/10 ------\n",
            "Total number of filters before pruning: 4020.0\n",
            "Total number of filters after pruning: 3822.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 2.2045 Acc: 0.2000\n",
            "val Loss: 2.3056 Acc: 0.1700\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 2.1768 Acc: 0.1875\n",
            "val Loss: 2.1025 Acc: 0.2100\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 2.1381 Acc: 0.1925\n",
            "val Loss: 2.1918 Acc: 0.2000\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 2.1541 Acc: 0.2175\n",
            "val Loss: 2.1238 Acc: 0.2600\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 2.1451 Acc: 0.2125\n",
            "val Loss: 2.0509 Acc: 0.2700\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 2.1040 Acc: 0.2225\n",
            "val Loss: 2.2304 Acc: 0.1800\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 2.0828 Acc: 0.2400\n",
            "val Loss: 2.0835 Acc: 0.2100\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 1.9663 Acc: 0.2675\n",
            "val Loss: 2.0526 Acc: 0.2500\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 1.9316 Acc: 0.2775\n",
            "val Loss: 1.9771 Acc: 0.2800\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 1.9265 Acc: 0.2625\n",
            "val Loss: 1.9943 Acc: 0.2700\n",
            "\n",
            "Training complete in 2m 36s\n",
            "Best val Acc: 0.280000\n",
            "\n",
            "------ Outer iteration 4/10 ------\n",
            "Total number of filters before pruning: 3822.0\n",
            "Total number of filters after pruning: 3637.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 2.0821 Acc: 0.2200\n",
            "val Loss: 2.1597 Acc: 0.2900\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 2.1260 Acc: 0.2225\n",
            "val Loss: 2.2752 Acc: 0.1800\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 2.1502 Acc: 0.1975\n",
            "val Loss: 2.0802 Acc: 0.2600\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 2.1138 Acc: 0.2175\n",
            "val Loss: 2.1556 Acc: 0.2200\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 2.0948 Acc: 0.2275\n",
            "val Loss: 2.2603 Acc: 0.2000\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 2.0981 Acc: 0.2300\n",
            "val Loss: 2.0504 Acc: 0.2900\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 2.0740 Acc: 0.2250\n",
            "val Loss: 2.2069 Acc: 0.2400\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 1.9340 Acc: 0.2450\n",
            "val Loss: 1.9739 Acc: 0.3300\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 1.9636 Acc: 0.2650\n",
            "val Loss: 1.9944 Acc: 0.2700\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 1.9265 Acc: 0.2850\n",
            "val Loss: 2.0894 Acc: 0.2300\n",
            "\n",
            "Training complete in 2m 24s\n",
            "Best val Acc: 0.330000\n",
            "\n",
            "------ Outer iteration 5/10 ------\n",
            "Total number of filters before pruning: 3637.0\n",
            "Total number of filters after pruning: 3458.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 2.0873 Acc: 0.2175\n",
            "val Loss: 2.1184 Acc: 0.1700\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 2.0669 Acc: 0.2400\n",
            "val Loss: 2.1424 Acc: 0.1600\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 2.0263 Acc: 0.2450\n",
            "val Loss: 2.2066 Acc: 0.2000\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 2.0383 Acc: 0.1975\n",
            "val Loss: 2.1414 Acc: 0.2100\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 2.0590 Acc: 0.2650\n",
            "val Loss: 2.1627 Acc: 0.2000\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 2.0535 Acc: 0.2275\n",
            "val Loss: 2.0070 Acc: 0.2100\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 2.0545 Acc: 0.2300\n",
            "val Loss: 1.9585 Acc: 0.2500\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 1.9709 Acc: 0.2775\n",
            "val Loss: 2.0517 Acc: 0.1800\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 1.8926 Acc: 0.2600\n",
            "val Loss: 2.0139 Acc: 0.1900\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 1.9356 Acc: 0.2550\n",
            "val Loss: 1.9494 Acc: 0.2000\n",
            "\n",
            "Training complete in 2m 19s\n",
            "Best val Acc: 0.250000\n",
            "\n",
            "------ Outer iteration 6/10 ------\n",
            "Total number of filters before pruning: 3458.0\n",
            "Total number of filters after pruning: 3294.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 2.1409 Acc: 0.1925\n",
            "val Loss: 2.0862 Acc: 0.1700\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 2.0674 Acc: 0.2625\n",
            "val Loss: 1.9491 Acc: 0.3400\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 2.0822 Acc: 0.2450\n",
            "val Loss: 1.9866 Acc: 0.2300\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 2.0348 Acc: 0.2750\n",
            "val Loss: 1.9252 Acc: 0.3600\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 2.0415 Acc: 0.2425\n",
            "val Loss: 1.9661 Acc: 0.3400\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 2.0223 Acc: 0.2250\n",
            "val Loss: 2.1927 Acc: 0.2600\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 2.0191 Acc: 0.2925\n",
            "val Loss: 1.9972 Acc: 0.2600\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 1.9369 Acc: 0.2675\n",
            "val Loss: 1.9425 Acc: 0.3000\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 1.9123 Acc: 0.3125\n",
            "val Loss: 1.8842 Acc: 0.3500\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 1.8935 Acc: 0.2750\n",
            "val Loss: 1.9340 Acc: 0.3500\n",
            "\n",
            "Training complete in 2m 12s\n",
            "Best val Acc: 0.360000\n",
            "\n",
            "------ Outer iteration 7/10 ------\n",
            "Total number of filters before pruning: 3294.0\n",
            "Total number of filters after pruning: 3136.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 2.0304 Acc: 0.2400\n",
            "val Loss: 2.0495 Acc: 0.2000\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 2.0753 Acc: 0.2500\n",
            "val Loss: 2.0352 Acc: 0.2700\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.9435 Acc: 0.2700\n",
            "val Loss: 2.1684 Acc: 0.1800\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.9575 Acc: 0.2675\n",
            "val Loss: 1.9995 Acc: 0.3300\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 2.0127 Acc: 0.2400\n",
            "val Loss: 2.0130 Acc: 0.2000\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 1.9451 Acc: 0.2975\n",
            "val Loss: 2.1902 Acc: 0.1500\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 1.9608 Acc: 0.2925\n",
            "val Loss: 2.0951 Acc: 0.1900\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 1.8346 Acc: 0.3375\n",
            "val Loss: 1.8546 Acc: 0.2700\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 1.8861 Acc: 0.2875\n",
            "val Loss: 1.9814 Acc: 0.2300\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 1.7733 Acc: 0.3500\n",
            "val Loss: 1.9770 Acc: 0.2900\n",
            "\n",
            "Training complete in 2m 0s\n",
            "Best val Acc: 0.330000\n",
            "\n",
            "------ Outer iteration 8/10 ------\n",
            "Total number of filters before pruning: 3136.0\n",
            "Total number of filters after pruning: 2989.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 2.1151 Acc: 0.2250\n",
            "val Loss: 2.0366 Acc: 0.2500\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 2.0888 Acc: 0.2175\n",
            "val Loss: 1.9853 Acc: 0.2200\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 2.0167 Acc: 0.2575\n",
            "val Loss: 2.0860 Acc: 0.2300\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 2.0136 Acc: 0.2975\n",
            "val Loss: 2.0858 Acc: 0.2700\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 2.0311 Acc: 0.2275\n",
            "val Loss: 2.1749 Acc: 0.1400\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 1.9956 Acc: 0.2700\n",
            "val Loss: 2.1537 Acc: 0.2400\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 1.9898 Acc: 0.2500\n",
            "val Loss: 2.0619 Acc: 0.2100\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 1.8757 Acc: 0.3025\n",
            "val Loss: 1.9226 Acc: 0.2900\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 1.8228 Acc: 0.3000\n",
            "val Loss: 2.0366 Acc: 0.2500\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 1.7732 Acc: 0.3675\n",
            "val Loss: 1.9504 Acc: 0.2700\n",
            "\n",
            "Training complete in 1m 55s\n",
            "Best val Acc: 0.290000\n",
            "\n",
            "------ Outer iteration 9/10 ------\n",
            "Total number of filters before pruning: 2989.0\n",
            "Total number of filters after pruning: 2842.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 2.0406 Acc: 0.2475\n",
            "val Loss: 2.0314 Acc: 0.2800\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 2.0387 Acc: 0.2650\n",
            "val Loss: 2.1204 Acc: 0.3100\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 2.0883 Acc: 0.2300\n",
            "val Loss: 1.9162 Acc: 0.2400\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.9771 Acc: 0.2725\n",
            "val Loss: 1.8070 Acc: 0.3800\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 2.0180 Acc: 0.2325\n",
            "val Loss: 1.8669 Acc: 0.3300\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 1.9207 Acc: 0.2925\n",
            "val Loss: 2.1515 Acc: 0.2200\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 1.9307 Acc: 0.2975\n",
            "val Loss: 2.0194 Acc: 0.3100\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 1.9130 Acc: 0.2575\n",
            "val Loss: 1.8771 Acc: 0.3400\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 1.8253 Acc: 0.3000\n",
            "val Loss: 1.7297 Acc: 0.4200\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 1.7945 Acc: 0.3350\n",
            "val Loss: 1.7307 Acc: 0.4200\n",
            "\n",
            "Training complete in 1m 49s\n",
            "Best val Acc: 0.420000\n",
            "\n",
            "------ Outer iteration 10/10 ------\n",
            "Total number of filters before pruning: 2842.0\n",
            "Total number of filters after pruning: 2704.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 2.0284 Acc: 0.2575\n",
            "val Loss: 1.8540 Acc: 0.3000\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.9300 Acc: 0.3075\n",
            "val Loss: 1.9930 Acc: 0.1900\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.9030 Acc: 0.2950\n",
            "val Loss: 2.1170 Acc: 0.2500\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.9035 Acc: 0.2775\n",
            "val Loss: 1.8482 Acc: 0.3300\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 1.8580 Acc: 0.3075\n",
            "val Loss: 1.8164 Acc: 0.3400\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 1.9500 Acc: 0.2950\n",
            "val Loss: 1.9923 Acc: 0.3000\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 1.8647 Acc: 0.3250\n",
            "val Loss: 1.9796 Acc: 0.3300\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 1.8430 Acc: 0.3400\n",
            "val Loss: 1.9333 Acc: 0.3900\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 1.7831 Acc: 0.3400\n",
            "val Loss: 1.9868 Acc: 0.2800\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 1.7459 Acc: 0.3950\n",
            "val Loss: 1.8808 Acc: 0.3100\n",
            "\n",
            "Training complete in 1m 45s\n",
            "Best val Acc: 0.390000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v7ORnILPDbVu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ]
    },
    {
      "metadata": {
        "id": "Icv-APyJDdB5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "aebfb7ad-f8d4-4afb-f95a-0f64f854e5b4"
      },
      "cell_type": "code",
      "source": [
        "# ====== Plot ======\n",
        "\n",
        "# ------ Loss ------\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(1, N_iter_outer*N_iter_inner+1), \n",
        "         prune_settings.epoch_loss, \n",
        "         color='red', \n",
        "         marker='',  markersize=12, \n",
        "         linestyle='-', linewidth=2,\n",
        "         label='Epoch loss')\n",
        "plt.plot(np.arange(1, N_iter_outer*N_iter_inner+1), \n",
        "         prune_settings.val_loss, \n",
        "         color='blue', \n",
        "         marker='',  markersize=12, \n",
        "         linestyle='-', linewidth=2,\n",
        "         label='Validation loss')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "# ------ Accuracy ------\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(1, N_iter_outer*N_iter_inner+1), \n",
        "         np.asarray(prune_settings.epoch_acc)*100.0, \n",
        "         color='red', \n",
        "         marker='',  markersize=12, \n",
        "         linestyle='-', linewidth=2,\n",
        "         label='Epoch accuracy')\n",
        "plt.plot(np.arange(1, N_iter_outer*N_iter_inner+1), \n",
        "         np.asarray(prune_settings.val_acc)*100.0, \n",
        "         color='blue', \n",
        "         marker='',  markersize=12, \n",
        "         linestyle='-', linewidth=2,\n",
        "         label = 'Validation accuracy')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.grid()\n",
        "plt.legend()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f7b900906d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhwYFpZRSDg0KSimlHP8fOneePOyWbsMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "UQlUKtFYD5pc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Routines"
      ]
    },
    {
      "metadata": {
        "id": "VASgSU9KD-81",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Baseline Model"
      ]
    },
    {
      "metadata": {
        "id": "yNR6vmDhEBT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model_baseline.train()\n",
        "\n",
        "model_baseline = train_model(model_baseline, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_wR099MzELCI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Pruned Model"
      ]
    },
    {
      "metadata": {
        "id": "0eNeKM87EN6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}