{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metapruning_units.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "tYqrMVdpA1NX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision as tv\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.modules import Module\n",
        "import torchvision.models.vgg as tv_vgg\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.modules.utils import _pair as pair\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BhzbK8ntAmnN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Computation Routines"
      ]
    },
    {
      "metadata": {
        "id": "NuhlcxSIA94G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ]
    },
    {
      "metadata": {
        "id": "YC8SpzkJA8gK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DatasetManager:\n",
        "    \n",
        "    def __init__(self, dataset='cifar10', percent_data=10.0, percent_val=20.0, data_path='./data'):\n",
        "        \n",
        "        # 'dataset' can be 'cifar10', 'cifar100', 'mnist', 'fashionmnist', 'kmnist', 'emnist', 'stl10', 'svhn'.\n",
        "        # 'percent_data' is the percentage of the full training set to be used.\n",
        "        # 'percent_val' is the percentage of the *loaded* training set to be used as validation data.\n",
        "        \n",
        "        data_path = './data/{}'.format(dataset)\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.data_path = data_path\n",
        "        self.percent_data = percent_data\n",
        "        self.percent_val = percent_val\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "            \n",
        "        elif self.dataset == 'cifar10' or\\\n",
        "             self.dataset == 'cifar100' or\\\n",
        "             self.dataset == 'stl10' or\\\n",
        "             self.dataset == 'svhn':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "        \n",
        "        elif self.dataset == 'mnist' or\\\n",
        "             self.dataset == 'fashionmnist' or\\\n",
        "             self.dataset == 'kmnist' or\\\n",
        "             self.dataset == 'emnist':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "        return\n",
        "    \n",
        "    \n",
        "    def ImportDataset(self, batch_size=5):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "        \n",
        "            self.trainset = tv.datasets.ImageFolder(root=self.data_path,\n",
        "                             transform=self.transform)\n",
        "        \n",
        "        # todo\n",
        "        \n",
        "        elif self.dataset == 'cifar10':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR10(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR10(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "        \n",
        "        elif self.dataset == 'cifar100':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR100(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR100(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "             \n",
        "        elif self.dataset == 'mnist':\n",
        "\n",
        "            self.trainset = tv.datasets.MNIST(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.MNIST(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'fashionmnist':\n",
        "\n",
        "            self.trainset = tv.datasets.FashionMNIST(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.FashionMNIST(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'kmnist':\n",
        "\n",
        "            self.trainset = tv.datasets.KMNIST(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.KMNIST(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'emnist':\n",
        "\n",
        "            self.trainset = tv.datasets.EMNIST(root=self.data_path, split='balanced', train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.EMNIST(root=self.data_path, split='balanced', train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'stl10':\n",
        "\n",
        "            self.trainset = tv.datasets.STL10(root=self.data_path, split='train',\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.STL10(root=self.data_path, split='test',\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'svhn':\n",
        "\n",
        "            self.trainset = tv.datasets.SVHN(root=self.data_path, split='train',\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.SVHN(root=self.data_path, split='test',\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        self.SplitData();\n",
        "        self.GenerateLoaders();\n",
        "                \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def SplitData(self):\n",
        "        \n",
        "        len_full = self.trainset.__len__()\n",
        "        len_train = int(np.round(len_full*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.trainset = torch.utils.data.random_split(self.trainset, (len_full-len_train, len_train))\n",
        "        \n",
        "        len_val = int(np.round(len_train*self.percent_val/100.0))\n",
        "        len_train = len_train - len_val\n",
        "        \n",
        "        self.valset, self.trainset = torch.utils.data.random_split(self.trainset, (len_val, len_train))\n",
        "         \n",
        "        len_full_test = self.testset.__len__()\n",
        "        len_test = int(np.round(len_full_test*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.testset = torch.utils.data.random_split(self.testset, (len_full_test-len_test, len_test))\n",
        "\n",
        "        print('\\nFull training set size: {}'.format(len_full))\n",
        "        print('Full test set size: {}'.format(len_full_test))\n",
        "        print('\\nActive training set size: {}'.format(len_train))\n",
        "        print('Active validation set size: {}'.format(len_val))\n",
        "        print('Active test set size: {}\\n'.format(len_test))\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def GenerateLoaders(self):\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.val_loader = torch.utils.data.DataLoader(self.valset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)          \n",
        "            \n",
        "        return\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUJWD_oDBKHD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training functions"
      ]
    },
    {
      "metadata": {
        "id": "34G6XlaFBMUW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def RecordLosses(phase, epoch_loss, epoch_acc, prune_settings):\n",
        "    \n",
        "    # Record losses for later use, plotting etc\n",
        "    if phase == 'train':\n",
        "        prune_settings.epoch_loss.append(epoch_loss)\n",
        "        prune_settings.epoch_acc.append(epoch_acc)\n",
        "    elif phase == 'val':\n",
        "        prune_settings.val_loss.append(epoch_loss)\n",
        "        prune_settings.val_acc.append(epoch_acc)\n",
        "\n",
        "    return prune_settings\n",
        "\n",
        "\n",
        "def PlotResults(prune_settings, savename=\"fig\"):\n",
        "    \n",
        "    # ====== Plot ======\n",
        "\n",
        "    # ------ Loss ------\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(1, len(prune_settings.epoch_loss)+1), \n",
        "             prune_settings.epoch_loss, \n",
        "             color='red', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Epoch loss')\n",
        "    plt.plot(np.arange(1, len(prune_settings.val_loss)+1), \n",
        "             prune_settings.val_loss, \n",
        "             color='blue', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Validation loss')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    savename_full = \"{}_loss.pdf\".format(savename)\n",
        "    plt.savefig(savename_full)\n",
        "    files.download(savename_full)\n",
        "\n",
        "    # ------ Accuracy ------\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(1, len(prune_settings.epoch_acc)+1), \n",
        "             np.asarray(prune_settings.epoch_acc)*100.0, \n",
        "             color='red', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Epoch accuracy')\n",
        "    plt.plot(np.arange(1, len(prune_settings.val_acc)+1), \n",
        "             np.asarray(prune_settings.val_acc)*100.0, \n",
        "             color='blue', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label = 'Validation accuracy')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    savename_full = \"{}_acc.pdf\".format(savename)\n",
        "    plt.savefig(savename_full)\n",
        "    files.download(savename_full)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def SaveResults(prune_settings, model):\n",
        "    \n",
        "    \n",
        "    \n",
        "    return\n",
        "\n",
        "\n",
        "def train_model(model, dat, criterion, optimizer, scheduler, prune_settings=0, num_epochs=25):\n",
        "    \n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                dataloader = dat.train_loader\n",
        "                dataset_size = dat.trainset.__len__()\n",
        "                \n",
        "                model.train()  # Set model to training mode\n",
        "                \n",
        "            else:\n",
        "                \n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                dataloader = dat.val_loader\n",
        "                dataset_size = dat.valset.__len__()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloader:\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                if prune_settings != 0:\n",
        "                    TrackConv2DNorms(model, prune_settings, inputs)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if training\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                \n",
        "            # Record losses for later use, plotting etc\n",
        "            prune_settings = RecordLosses(phase, epoch_loss, epoch_acc, prune_settings)\n",
        "\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # Record losses for later use, plotting etc\n",
        "    prune_settings.outer_iter_time.append(time_elapsed)\n",
        "    \n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z7C3UubBBXFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Filter Pruning functions"
      ]
    },
    {
      "metadata": {
        "id": "MaSebsFceMRD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning settings"
      ]
    },
    {
      "metadata": {
        "id": "jAZVF5O-BZNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Constants that define possible pruning metrics\n",
        "WEIGHT_NORM = 1\n",
        "ACT_NORM = 2\n",
        "\n",
        "# Class that contains various settings pertaining to how filters are pruned\n",
        "class UnitPruningSettings:\n",
        "    \n",
        "    def __init__(self, idx_layer=0, idx_filter=0, N_prune=1, P_prune=10, p=2, pruning_metric=WEIGHT_NORM):\n",
        "        \n",
        "        # EITHER N_prune OR P_prune will be used to decide how many filters to prune.\n",
        "        # If one is non-positive, the other is used.\n",
        "        # If neither is non-positive, priority is given to P_prune.\n",
        "        # If both are non-positive, no pruning will happen.\n",
        "\n",
        "        self.N_prune = N_prune # Number of filters allowed to be pruned in one pass\n",
        "        self.P_prune = P_prune; # Percent of filters of the current layer to prune\n",
        "        \n",
        "        self.idx_filter = idx_filter # Indices of the N_prune filters\n",
        "        self.idx_layer = idx_layer # Current layer under consideration\n",
        "        self.p = p # p-norm to use when computing which filters to remove\n",
        "        self.pruning_metric = pruning_metric\n",
        "        \n",
        "        self.norms_botk = []\n",
        "        self.idx_norms_botk = []\n",
        "        \n",
        "        # Various statistics will be stored and computed to keep track of how the network changes\n",
        "        \n",
        "        # Number of filters per layer in the original network\n",
        "        self.filters_per_layer_orig = []\n",
        "        \n",
        "        # Number of filters per layer after pruning - this gets updated every time the network is pruned\n",
        "        self.filters_per_layer_after = []\n",
        "        \n",
        "        # Time taken to prune in sec (running total, updated every time pruning happens)\n",
        "        self.prune_time = 0.0\n",
        "        self.outer_iter_time = []\n",
        "        \n",
        "        # Keep track of running epoch loss and validation loss, and corresponding accuracy\n",
        "        self.epoch_loss = []\n",
        "        self.val_loss = []\n",
        "        self.epoch_acc = []\n",
        "        self.val_acc = []\n",
        "        \n",
        "        return\n",
        "    \n",
        "    # Function to print the current pruning state of the model. Verbose can be 0, 1, or 2.\n",
        "    def PrintPruningStatistics(self, verbose=1):\n",
        "    \n",
        "        if verbose == 0:\n",
        "            return\n",
        "        \n",
        "        print(\"Total number of filters before pruning: {}\".format(sum(self.filters_per_layer_orig)))\n",
        "        print(\"Total number of filters after pruning: {}\".format(sum(self.filters_per_layer_after)))\n",
        "    \n",
        "        return\n",
        "    \n",
        "    # Function to set up and initialize based on a given model\n",
        "    def Setup(self, model):\n",
        "        \n",
        "        # Count the number of conv layers\n",
        "        self.N_layers = 0\n",
        "        \n",
        "        for layer, (name, module) in enumerate(model.features._modules.items()):\n",
        "            self.N_layers += 1\n",
        "                    \n",
        "        # Initialize storage containers\n",
        "        self.norms_botk = [None]*self.N_layers\n",
        "        self.idx_norms_botk = [None]*self.N_layers\n",
        "        \n",
        "    \n",
        "    # Function to reset norm containers\n",
        "    def ResetNormContainers(self):\n",
        "    \n",
        "        self.norms_botk = [None]*self.N_layers\n",
        "        self.idx_norms_botk = [None]*self.N_layers\n",
        "    \n",
        "        return\n",
        "\n",
        "    # Function to reset filter containers\n",
        "    def ResetFilterContainers(self):\n",
        "    \n",
        "        self.filters_per_layer_orig = []\n",
        "        self.filters_per_layer_after = []\n",
        "    \n",
        "        return\n",
        "\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qc41Ng14ezag",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning decisions"
      ]
    },
    {
      "metadata": {
        "id": "2F0n6VUTe2ND",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to compute the p-norm of weights in all filters of a given layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DWeightNorms(model, idx_layer, p):\n",
        "    \n",
        "    # Extract the layer of the model currently being considered\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    weights = conv.weight.data\n",
        "\n",
        "    # Compute norms of each filter\n",
        "    norms = weights.norm(p, dim=2).norm(p, dim=2).norm(p, dim=1)\n",
        "#     norms = norms/torch.max(torch.abs(norms))\n",
        "    \n",
        "    return norms\n",
        "\n",
        "\n",
        "# Function to compute the p-norm of activations in all filters per layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DActNorms(activation, prune_settings):\n",
        "    \n",
        "    p = prune_settings.p\n",
        "    \n",
        "    # Compute norms of each activation\n",
        "    norms = torch.norm(activation, p, dim=0).norm(p, dim=1).norm(p, dim=1)\n",
        "#     norms = norms/torch.max(torch.abs(norms))\n",
        "        \n",
        "    return norms\n",
        "\n",
        "\n",
        "# Function to track the p-norm of activations of all filters of during training.\n",
        "def TrackConv2DNorms(model, prune_settings, inputs):\n",
        "    \n",
        "    p = prune_settings.p\n",
        "    P_prune = prune_settings.P_prune\n",
        "\n",
        "    x = Variable(inputs)\n",
        "\n",
        "    ii = -1\n",
        "    for layer, (name, module) in enumerate(model.features._modules.items()):\n",
        "        ii += 1\n",
        "        x = module(x)\n",
        "        \n",
        "        if isinstance(module, torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            if prune_settings.pruning_metric == WEIGHT_NORM:\n",
        "                norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "            elif prune_settings.pruning_metric == ACT_NORM:\n",
        "                norms = ComputeConv2DActNorms(x, prune_settings)\n",
        "\n",
        "            # Use the given prune percentage to figure out how many filters to prune\n",
        "            if (P_prune >= 0):\n",
        "                N_prune = int(len(norms.float())*P_prune/100.0)\n",
        "                prune_settings.N_prune = N_prune\n",
        "\n",
        "#             n_botk, ind_botk = torch.topk(norms, N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            norms = norms.cpu().detach().numpy()\n",
        "    \n",
        "            # Store the norms for each filter\n",
        "#             if prune_settings.norms_botk[ii] is None:\n",
        "#                 prune_settings.norms_botk[ii] = norms\n",
        "#             else:\n",
        "#                 prune_settings.norms_botk[ii] += norms\n",
        "                \n",
        "            # Store normalized norms for each filter\n",
        "            if prune_settings.norms_botk[ii] is None:\n",
        "                prune_settings.norms_botk[ii] = norms/max(norms)\n",
        "            else:\n",
        "                prune_settings.norms_botk[ii] += norms/max(norms)\n",
        "                \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U59vXkzfehDX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning workers"
      ]
    },
    {
      "metadata": {
        "id": "RY5GZWxXej1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The following functions were adapted from https://github.com/jacobgil/pytorch-pruning/blob/master/prune.py\n",
        "\n",
        "def replace_layers(model, i, idx, layers):\n",
        "\tif i in idx:\n",
        "\t\treturn layers[idx.index(i)]\n",
        "\treturn model[i]\n",
        "\n",
        "\n",
        "# Function to prune a given convolution layer in the model provided.\n",
        "# Input \"idx_layers\" is the global index of the convolution layer to be pruned.\n",
        "# Input \"prune_settings\" is a data structure containing information on how pruning is performed.\n",
        "def PruneConvLayers(model, prune_settings):\n",
        "    \n",
        "    # Strategy: in order to prune a particular layer, the output of the previous layer \n",
        "    # and the inputs to the next layer must also be altered accordingly.\n",
        "\t\n",
        "    # Extract pruning settings for convenience\n",
        "    N_prune = prune_settings.N_prune\n",
        "    idx_filter = prune_settings.idx_filter\n",
        "    idx_layer = prune_settings.idx_layer\n",
        "    \n",
        "    if idx_layer >= len(model.features._modules.items()):\n",
        "        return\n",
        "        \n",
        "    # Extract the layer of the model currently being pruned\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    \n",
        "\n",
        "    # In case the list of target filters to delete has out-of-range entries, detect and ignore them\n",
        "    del_filters = []\n",
        "    for kk in range(0, len(idx_filter)):\n",
        "        if idx_filter[kk] >= conv.out_channels:\n",
        "            del_filters.extend(kk)\n",
        "    \n",
        "    if (len(del_filters) > 0):\n",
        "        idx_filter = np.delete(idx_filter, del_filters, 0)\n",
        "        N_prune = len(idx_filter)\n",
        "        prune_settings.N_prune = N_prune\n",
        "        print(\"[WARNING] Encountered an out-of-range target filter; it will be ignored.\")\n",
        "    \n",
        "    # Record pruning statistics\n",
        "    prune_settings.filters_per_layer_orig[idx_layer] = conv.out_channels\n",
        "    prune_settings.filters_per_layer_after[idx_layer] = conv.out_channels - N_prune\n",
        "    \n",
        "        \n",
        "    # To keep track of the succeeding convolution layer\n",
        "    next_conv = None\n",
        "    offset = 1\n",
        "    \n",
        "    # Figure out how many layers after this one are NOT conv layers, in order to skip pruning them\n",
        "    while idx_layer + offset < len(model.features._modules.items()):\n",
        "        \n",
        "        res =  list(model.features._modules.items())[idx_layer + offset]\n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            next_name, next_conv = res\n",
        "            break\n",
        "        offset = offset + 1\n",
        "    \n",
        "    # Create a new, replacement conv layer to remove a given number of filters.\n",
        "    # The rest of its settings should remain the same as the original conv layer.\n",
        "    new_conv = torch.nn.Conv2d(in_channels = conv.in_channels,\n",
        "                               out_channels = conv.out_channels - N_prune,\n",
        "\t\t\t                   kernel_size = conv.kernel_size,\n",
        "                               stride = conv.stride,\n",
        "                               padding = conv.padding,\n",
        "                               dilation = conv.dilation,\n",
        "                               groups = conv.groups,\n",
        "                               bias = True)\n",
        "    \n",
        "    new_conv.bias = conv.bias\n",
        "    \n",
        "    # Copy over the weights to the new conv layer, except the ones corresponding to the filter to be removed\n",
        "    old_weights = conv.weight.data.cpu().numpy()\n",
        "    new_weights = new_conv.weight.data.cpu().numpy()\n",
        "    \n",
        "    # Copy over the set of filters, excluding the ones to be removed\n",
        "    new_weights_temp = np.copy(old_weights)\n",
        "    new_weights_temp = np.delete(new_weights_temp, idx_filter, 0)\n",
        "    new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "    # Update weight data of the new conv layer\n",
        "    new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "    \n",
        "    # Now do the same thing for biases\n",
        "    old_biases = conv.bias.data.cpu().numpy()\n",
        "    new_biases = np.zeros(shape=(old_biases.shape[0] - N_prune), dtype=np.float32)\n",
        "    \n",
        "    new_biases_temp = np.copy(old_biases)\n",
        "    new_biases_temp = np.delete(new_biases_temp, idx_filter, 0)\n",
        "    new_biases[:] = new_biases_temp[:]\n",
        "        \n",
        "    new_conv.bias.data = torch.from_numpy(new_biases).cuda()\n",
        "    \n",
        "    # If there is a succeeding conv layer, adjust its input units and weights accordingly\n",
        "    if next_conv != None:\n",
        "        \n",
        "        next_new_conv = torch.nn.Conv2d(in_channels = next_conv.in_channels - N_prune,\n",
        "                                        out_channels =  next_conv.out_channels,\n",
        "                                        kernel_size = next_conv.kernel_size,\n",
        "                                        stride = next_conv.stride,\n",
        "                                        padding = next_conv.padding,\n",
        "                                        dilation = next_conv.dilation,\n",
        "                                        groups = next_conv.groups,\n",
        "                                        bias = True)\n",
        "        \n",
        "        next_new_conv.bias = next_conv.bias\n",
        "\n",
        "        old_weights = next_conv.weight.data.cpu().numpy()\n",
        "        new_weights = next_new_conv.weight.data.cpu().numpy()\n",
        "        \n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_filter, 1)\n",
        "        new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "        next_new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        # Now do the same thing for biases\n",
        "        next_new_conv.bias.data = next_conv.bias.data\n",
        "\n",
        "        # Update the actual model by replacing the existing filters with the new ones\n",
        "        features = torch.nn.Sequential(\n",
        "                *(replace_layers(model.features, i, [idx_layer, idx_layer + offset], \\\n",
        "                    [new_conv, next_new_conv]) for i, _ in enumerate(model.features)))\n",
        "        del model.features\n",
        "        del conv\n",
        "\n",
        "        model.features = features\n",
        "    \n",
        "    else:\n",
        "\n",
        "        # This is the last conv layer. This affects the first linear layer of the classifier.\n",
        "        model.features = torch.nn.Sequential(*(replace_layers(model.features, i, [idx_layer], [new_conv]) for i, _ in enumerate(model.features)))\n",
        "        idx_layer = 0\n",
        "        old_linear_layer = None\n",
        "\n",
        "        for _, module in model.classifier._modules.items():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                old_linear_layer = module\n",
        "                break\n",
        "            idx_layer = idx_layer + 1\n",
        "\n",
        "        if old_linear_layer == None:\n",
        "            raise BaseException(\"No linear layer found in classifier.\")\n",
        "            \n",
        "        params_per_input_channel = int(old_linear_layer.in_features/conv.out_channels)\n",
        "\n",
        "        new_linear_layer = torch.nn.Linear(old_linear_layer.in_features - N_prune*params_per_input_channel, \n",
        "                                           old_linear_layer.out_features)\n",
        "\n",
        "        old_weights = old_linear_layer.weight.data.cpu().numpy()\n",
        "        new_weights = new_linear_layer.weight.data.cpu().numpy()\t \t\n",
        "\n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        idx_expanded = np.zeros(shape=(N_prune*params_per_input_channel))\n",
        "        \n",
        "        for kk in range(0, len(idx_filter)):\n",
        "            idx_expanded[kk*params_per_input_channel:kk*params_per_input_channel+params_per_input_channel] = np.arange(idx_filter[kk]*params_per_input_channel, idx_filter[kk]*params_per_input_channel + params_per_input_channel)\n",
        "\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_expanded.astype(int), 1)\n",
        "        new_weights[:, :] = new_weights_temp[:, :]\n",
        "        \n",
        "        new_linear_layer.bias.data = old_linear_layer.bias.data\n",
        "        new_linear_layer.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        classifier = torch.nn.Sequential(*(replace_layers(model.classifier, i, [idx_layer], [new_linear_layer]) for i, _ in enumerate(model.classifier)))\n",
        "\n",
        "        del model.classifier\n",
        "        del next_conv\n",
        "        del conv\n",
        "        model.classifier = classifier\n",
        "        \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7h5EFGhe-PJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning driver"
      ]
    },
    {
      "metadata": {
        "id": "Xp8V5o_UfADH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to iterate through all conv2D layers of the network and determine \n",
        "# filters to be pruned, and then carry out the pruning.\n",
        "def PruneAllConv2DLayers(model, prune_settings):\n",
        "    \n",
        "    # Extract pruning settings for convenience\n",
        "    # Note that \"N_prune\" *consecutive* filters will get pruned\n",
        "    N_prune = prune_settings.N_prune\n",
        "    P_prune = prune_settings.P_prune\n",
        "    p = prune_settings.p\n",
        "    pruning_metric = prune_settings.pruning_metric\n",
        "    \n",
        "    # Count number of prunable layers for preallocation\n",
        "    N_layers = len(model.features._modules.items())       \n",
        "    prune_settings.filters_per_layer_orig = np.zeros(shape=(1, N_layers)).ravel()\n",
        "    prune_settings.filters_per_layer_after = np.zeros(shape=(1, N_layers)).ravel()\n",
        "\n",
        "    \n",
        "    # Find the N_prune filters to remove\n",
        "    ii = 0\n",
        "    while ii < len(model.features._modules.items()):\n",
        "        \n",
        "        res = list(model.features._modules.items())[ii]\n",
        "        \n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            _, conv = list(model.features._modules.items())[ii]\n",
        "            \n",
        "            # Record pruning statistics\n",
        "            prune_settings.filters_per_layer_orig[ii] = conv.out_channels\n",
        "            prune_settings.filters_per_layer_after[ii] = conv.out_channels\n",
        "            \n",
        "            # Compute values and indices of the N_prune smallest norms\n",
        "#             if pruning_metric == WEIGHT_NORM:\n",
        "#                 norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "#             elif pruning_metric == ACT_NORM:\n",
        "# #                 norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "#                 norms = ComputeConv2DActNorms(res[1], prune_settings)\n",
        "                \n",
        "        \n",
        "            if (P_prune >= 0):\n",
        "                N_prune = int(conv.out_channels*P_prune/100.0)\n",
        "                prune_settings.N_prune = N_prune\n",
        "            \n",
        "            if prune_settings.norms_botk[ii] is not None:\n",
        "                \n",
        "#                 n_botk, ind_botk = torch.topk(torch.from_numpy(prune_settings.norms_botk[ii]), N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            \n",
        "                norms = np.asarray(prune_settings.norms_botk[ii]).ravel()\n",
        "                ind_botk = np.argpartition(norms, N_prune)    \n",
        "                n_botk = norms[ind_botk[:N_prune]]\n",
        "                ind_botk = ind_botk[:N_prune]\n",
        "        \n",
        "                prune_settings.idx_layer = ii\n",
        "                prune_settings.idx_filter = ind_botk\n",
        "\n",
        "                model = PruneConvLayers(model, prune_settings)\n",
        "                \n",
        "        ii = ii + 1\n",
        "            \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cCEASFCwB3uP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Pruning"
      ]
    },
    {
      "metadata": {
        "id": "LASRjk-I2zXD",
        "colab_type": "code",
        "outputId": "603778d6-a802-4b56-8d37-bc6819152695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# Test pruning\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.train()\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10, 12, 15, 16, 21), \n",
        "                                     N_prune=5, p=2, pruning_metric=WEIGHT_NORM)\n",
        "# prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10), \n",
        "#                                      N_prune=1, p=2, pruning_metric=WEIGHT_NORM)\n",
        "\n",
        "N_layers = len(model.features._modules.items())       \n",
        "prune_settings.filters_per_layer_orig = np.zeros(shape=(1, N_layers)).ravel()\n",
        "prune_settings.filters_per_layer_after = np.zeros(shape=(1, N_layers)).ravel()\n",
        "\n",
        "t0 = time.time()\n",
        "model = PruneConvLayers(model, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.torch/models/vgg16-397923af.pth\n",
            "553433881it [00:27, 20199209.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Pruning took 12.155690670013428 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BgOIBqly2ntw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# L0 Masking Functions"
      ]
    },
    {
      "metadata": {
        "id": "zEl8diRM2sa0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Masked:\n",
        "    def make_mask(self, threshold, mask=None):\n",
        "        if mask is None:\n",
        "            print(\"new mask\", device)\n",
        "            self.mask = torch.ones(self.weight.size(), requires_grad=False).to(device)\n",
        "        else:\n",
        "            self.mask = mask      \n",
        "        self.zeros = torch.zeros(self.weight.size(), requires_grad=False).to(device)\n",
        "        self.threshold = threshold\n",
        "    \n",
        "    def set_threshold(self, prop=0.05):\n",
        "        unique_weights = torch.unique(self.weight*self.mask)\n",
        "        mask_size = self.mask.reshape(-1).size()[0]\n",
        "#     mask_size = mask_size[0]*mask_size[1]\n",
        "        mask_nonzero = torch.sum(self.mask.view([mask_size]))\n",
        "        mask_total = mask_size\n",
        "        print('nonzero proportion: {:.4f}'.format(mask_nonzero/mask_total))\n",
        "        self.threshold = torch.max(torch.topk(torch.abs(unique_weights),int(prop*unique_weights.size()[0]),largest=False)[0])    \n",
        "\n",
        "    def make_threshold_mask(self):\n",
        "        self.mask = torch.where(torch.abs(self.weight) >= self.threshold,self.mask,self.zeros).to(device)\n",
        "#     self.mask.requires_grad_(requires_grad=False)\n",
        "    def mask_weight(self):\n",
        "        self.weight = torch.nn.Parameter(self.weight*self.mask).to(device)\n",
        "    \n",
        "class MaskedLinear(torch.nn.Linear, Masked):\n",
        "    def __init__(self, in_features, out_features, bias=True, threshold=0.001, mask=None):\n",
        "        super(MaskedLinear, self).__init__(in_features,out_features)\n",
        "        self.make_mask(threshold,mask)\n",
        "    def forward(self, input):\n",
        "        self.make_threshold_mask()\n",
        "        self.mask_weight()\n",
        "        #     print(self.mask[125:135,125:135])\n",
        "        #     print(self.weight[125:135,125:135])\n",
        "        return F.linear(input, self.weight, self.bias)\n",
        "\n",
        "class MaskedConv(torch.nn.Conv2d, Masked):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, groups, bias=True, threshold=0.0001):\n",
        "        super(MaskedConv,self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.make_mask(threshold)    \n",
        "    def forward(self, input):\n",
        "        self.mask_weight()\n",
        "        return F.conv2d(input, self.weight, self.bias, self.stride,\n",
        "                    self.padding, self.dilation, self.groups)\n",
        "\n",
        "limit_a, limit_b, epsilon = -.1, 1.1, 1e-6\n",
        "device='cuda'\n",
        "\n",
        "class LinearL0(Module):\n",
        "    \"\"\"Implementation of L0 regularization for the input units of a fully connected layer\"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True, weight_decay=1., droprate_init=0.5, temperature=2./3.,\n",
        "                 lamba=1., local_rep=False, **kwargs):\n",
        "        \"\"\"\n",
        "        :param in_features: Input dimensionality\n",
        "        :param out_features: Output dimensionality\n",
        "        :param bias: Whether we use a bias\n",
        "        :param weight_decay: Strength of the L2 penalty\n",
        "        :param droprate_init: Dropout rate that the L0 gates will be initialized to\n",
        "        :param temperature: Temperature of the concrete distribution\n",
        "        :param lamba: Strength of the L0 penalty\n",
        "        :param local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
        "        \"\"\"\n",
        "        super(LinearL0, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.prior_prec = weight_decay\n",
        "        self.weights = torch.nn.Parameter(torch.Tensor(in_features, out_features).to(device))\n",
        "        #         self.qz_loga = torch.Tensor(in_features).to(device)\n",
        "        self.qz_loga = torch.nn.Parameter(torch.Tensor(in_features).to(device))\n",
        "        self.temperature = temperature\n",
        "        self.droprate_init = droprate_init if droprate_init != 0. else 0.5\n",
        "        self.lamba = lamba\n",
        "        self.use_bias = False\n",
        "        self.local_rep = local_rep\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
        "            self.use_bias = True\n",
        "        self.floatTensor = torch.FloatTensor if not torch.cuda.is_available() else torch.cuda.FloatTensor\n",
        "        self.reset_parameters()\n",
        "        print(self)\n",
        "        \n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.kaiming_normal(self.weights, mode='fan_out')\n",
        "\n",
        "        self.qz_loga.data.normal_(math.log(1 - self.droprate_init) - math.log(self.droprate_init), 1e-2)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias.data.fill_(0)\n",
        "\n",
        "    def constrain_parameters(self, **kwargs):\n",
        "        self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
        "\n",
        "    def cdf_qz(self, x):\n",
        "        \"\"\"Implements the CDF of the 'stretched' concrete distribution\"\"\"\n",
        "        xn = (x - limit_a) / (limit_b - limit_a)\n",
        "        logits = math.log(xn) - math.log(1 - xn)\n",
        "        return F.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=epsilon, max=1 - epsilon).to(device)\n",
        "\n",
        "    def quantile_concrete(self, x):\n",
        "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "        y = F.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature).to(device)\n",
        "        return y * (limit_b - limit_a) + limit_a\n",
        "\n",
        "    def _reg_w(self):\n",
        "        \"\"\"Expected L0 norm under the stochastic gates, takes into account and re-weights also a potential L2 penalty\"\"\"\n",
        "        logpw_col = torch.sum(- (.5 * self.prior_prec * self.weights.pow(2)) - self.lamba, 1).to(device)\n",
        "        logpw = torch.sum((1 - self.cdf_qz(0)) * logpw_col).to(device)\n",
        "        logpb = 0 if not self.use_bias else - torch.sum(.5 * self.prior_prec * self.bias.pow(2)).to(device)\n",
        "        return logpw + logpb\n",
        "        \n",
        "        \n",
        "    def regularization(self):\n",
        "        return self._reg_w()\n",
        "\n",
        "    def count_expected_flops_and_l0(self):\n",
        "        \"\"\"Measures the expected floating point operations (FLOPs) and the expected L0 norm\"\"\"\n",
        "        # dim_in multiplications and dim_in - 1 additions for each output neuron for the weights\n",
        "        # + the bias addition for each neuron\n",
        "        # total_flops = (2 * in_features - 1) * out_features + out_features\n",
        "        ppos = torch.sum(1 - self.cdf_qz(0))\n",
        "        expected_flops = (2 * ppos - 1) * self.out_features\n",
        "        expected_l0 = ppos * self.out_features\n",
        "        if self.use_bias:\n",
        "            expected_flops += self.out_features\n",
        "            expected_l0 += self.out_features\n",
        "#       return expected_flops.data[0], expected_l0.data[0]\n",
        "        return expected_flops, expected_l0\n",
        "\n",
        "    def get_eps(self, size):\n",
        "        \"\"\"Uniform random numbers for the concrete distribution\"\"\"\n",
        "        eps = self.floatTensor(size).uniform_(epsilon, 1-epsilon).to(device)\n",
        "        eps = Variable(eps)\n",
        "        return eps\n",
        "\n",
        "    def sample_z(self, batch_size, sample=True):\n",
        "        \"\"\"Sample the hard-concrete gates for training and use a deterministic value for testing\"\"\"\n",
        "        if sample:\n",
        "            eps = self.get_eps(self.floatTensor(batch_size, self.in_features).to(device))\n",
        "            z = self.quantile_concrete(eps)\n",
        "            return F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "        else:  # mode\n",
        "            pi = F.sigmoid(self.qz_loga).view(1, self.in_features).expand(batch_size, self.in_features).to(device)\n",
        "            return F.hardtanh(pi * (limit_b - limit_a) + limit_a, min_val=0, max_val=1).to(device)\n",
        "        \n",
        "    def sample_weights(self):\n",
        "        z = self.quantile_concrete(self.get_eps(self.floatTensor(self.in_features).to(device)))\n",
        "        mask = F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "        return mask.view(self.in_features, 1) * self.weights\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.local_rep or not self.training:\n",
        "            z = self.sample_z(input.size(0), sample=self.training)\n",
        "            xin = input.mul(z)\n",
        "            output = xin.mm(self.weights)\n",
        "        else:\n",
        "            weights = self.sample_weights()\n",
        "            output = input.mm(weights)\n",
        "        if self.use_bias:\n",
        "            output.add_(self.bias)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = ('{name}({in_features} -> {out_features}, droprate_init={droprate_init}, '\n",
        "            'lamba={lamba}, temperature={temperature}, weight_decay={prior_prec}, '\n",
        "            'local_rep={local_rep}')\n",
        "        if not self.use_bias:\n",
        "            s += ', bias=False'\n",
        "        s += ')'\n",
        "        return s.format(name=self.__class__.__name__, **self.__dict__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ikIYcgE-FbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class L0Conv2d(Module):\n",
        "    \"\"\"Implementation of L0 regularization for the feature maps of a convolutional layer\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,\n",
        "                 droprate_init=0.5, temperature=2./3., weight_decay=1., lamba=1., local_rep=False, **kwargs):\n",
        "        \"\"\"\n",
        "        :param in_channels: Number of input channels\n",
        "        :param out_channels: Number of output channels\n",
        "        :param kernel_size: Size of the kernel\n",
        "        :param stride: Stride for the convolution\n",
        "        :param padding: Padding for the convolution\n",
        "        :param dilation: Dilation factor for the convolution\n",
        "        :param groups: How many groups we will assume in the convolution\n",
        "        :param bias: Whether we will use a bias\n",
        "        :param droprate_init: Dropout rate that the L0 gates will be initialized to\n",
        "        :param temperature: Temperature of the concrete distribution\n",
        "        :param weight_decay: Strength of the L2 penalty\n",
        "        :param lamba: Strength of the L0 penalty\n",
        "        :param local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
        "        \"\"\"\n",
        "        super(L0Conv2d, self).__init__()\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = pair(kernel_size)\n",
        "        self.stride = pair(stride)\n",
        "        self.padding = pair(padding)\n",
        "        self.dilation = pair(dilation)\n",
        "        self.output_padding = pair(0)\n",
        "        self.groups = groups\n",
        "        self.prior_prec = weight_decay\n",
        "        self.lamba = lamba\n",
        "        self.droprate_init = droprate_init if droprate_init != 0. else 0.5\n",
        "        self.temperature = temperature\n",
        "        self.floatTensor = torch.FloatTensor if not torch.cuda.is_available() else torch.cuda.FloatTensor\n",
        "        self.use_bias = False\n",
        "        self.weights = Parameter(torch.Tensor(out_channels, in_channels // groups, *self.kernel_size).to(device))\n",
        "        self.qz_loga = Parameter(torch.Tensor(out_channels).to(device))\n",
        "        self.dim_z = out_channels\n",
        "        self.input_shape = None\n",
        "        self.local_rep = local_rep\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels).to(device))\n",
        "            self.use_bias = True\n",
        "\n",
        "        self.reset_parameters()\n",
        "        print(self)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_normal(self.weights, mode='fan_in')\n",
        "\n",
        "        self.qz_loga.data.normal_(math.log(1 - self.droprate_init) - math.log(self.droprate_init), 1e-2)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias.data.fill_(0)\n",
        "\n",
        "    def constrain_parameters(self, **kwargs):\n",
        "        self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
        "\n",
        "    def cdf_qz(self, x):\n",
        "        \"\"\"Implements the CDF of the 'stretched' concrete distribution\"\"\"\n",
        "        xn = (x - limit_a) / (limit_b - limit_a)\n",
        "        logits = math.log(xn) - math.log(1 - xn)\n",
        "        return F.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=epsilon, max=1 - epsilon)\n",
        "\n",
        "    def quantile_concrete(self, x):\n",
        "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "        y = F.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature)\n",
        "        return y * (limit_b - limit_a) + limit_a\n",
        "\n",
        "    def _reg_w(self):\n",
        "        \"\"\"Expected L0 norm under the stochastic gates, takes into account and re-weights also a potential L2 penalty\"\"\"\n",
        "        q0 = self.cdf_qz(0)\n",
        "        logpw_col = torch.sum(- (.5 * self.prior_prec * self.weights.pow(2)) - self.lamba, 3).sum(2).sum(1)\n",
        "        logpw = torch.sum((1 - q0) * logpw_col).to(device)\n",
        "        logpb = 0 if not self.use_bias else - torch.sum((1 - q0) * (.5 * self.prior_prec * self.bias.pow(2) -\n",
        "                                                                    self.lamba))\n",
        "        return logpw + logpb\n",
        "\n",
        "    def regularization(self):\n",
        "        return self._reg_w()\n",
        "\n",
        "    def count_expected_flops_and_l0(self):\n",
        "        \"\"\"Measures the expected floating point operations (FLOPs) and the expected L0 norm\"\"\"\n",
        "        ppos = torch.sum(1 - self.cdf_qz(0))\n",
        "        n = self.kernel_size[0] * self.kernel_size[1] * self.in_channels  # vector_length\n",
        "        flops_per_instance = n + (n - 1)  # (n: multiplications and n-1: additions)\n",
        "\n",
        "        num_instances_per_filter = ((self.input_shape[1] - self.kernel_size[0] + 2 * self.padding[0]) / self.stride[0]) + 1  # for rows\n",
        "        num_instances_per_filter *= ((self.input_shape[2] - self.kernel_size[1] + 2 * self.padding[1]) / self.stride[1]) + 1  # multiplying with cols\n",
        "\n",
        "        flops_per_filter = num_instances_per_filter * flops_per_instance\n",
        "        expected_flops = flops_per_filter * ppos  # multiply with number of filters\n",
        "        expected_l0 = n * ppos\n",
        "\n",
        "        if self.use_bias:\n",
        "            # since the gate is applied to the output we also reduce the bias computation\n",
        "            expected_flops += num_instances_per_filter * ppos\n",
        "            expected_l0 += ppos\n",
        "\n",
        "#         return expected_flops.data[0], expected_l0.data[0]\n",
        "        return expected_flops, expected_l0\n",
        "\n",
        "    def get_eps(self, size):\n",
        "        \"\"\"Uniform random numbers for the concrete distribution\"\"\"\n",
        "        eps = self.floatTensor(size).uniform_(epsilon, 1-epsilon).to(device)\n",
        "        eps = Variable(eps)\n",
        "        return eps\n",
        "\n",
        "    def sample_z(self, batch_size, sample=True):\n",
        "        \"\"\"Sample the hard-concrete gates for training and use a deterministic value for testing\"\"\"\n",
        "        if sample:\n",
        "            eps = self.get_eps(self.floatTensor(batch_size, self.dim_z)).to(device)\n",
        "            z = self.quantile_concrete(eps).view(batch_size, self.dim_z, 1, 1)\n",
        "            return F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "        else:  # mode\n",
        "            pi = F.sigmoid(self.qz_loga).view(1, self.dim_z, 1, 1)\n",
        "            return F.hardtanh(pi * (limit_b - limit_a) + limit_a, min_val=0, max_val=1).to(device)\n",
        "\n",
        "    def sample_weights(self):\n",
        "        z = self.quantile_concrete(self.get_eps(self.floatTensor(self.dim_z).to(device))).view(self.dim_z, 1, 1, 1)\n",
        "        return F.hardtanh(z, min_val=0, max_val=1).to(device) * self.weights\n",
        "\n",
        "    def forward(self, input_):\n",
        "        if self.input_shape is None:\n",
        "            self.input_shape = input_.size()\n",
        "        b = None if not self.use_bias else self.bias\n",
        "        if self.local_rep or not self.training:\n",
        "            output = F.conv2d(input_, self.weights, b, self.stride, self.padding, self.dilation, self.groups)\n",
        "            z = self.sample_z(output.size(0), sample=self.training)\n",
        "            return output.mul(z)\n",
        "        else:\n",
        "            weights = self.sample_weights()\n",
        "            output = F.conv2d(input_, weights, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = ('{name}({in_channels}, {out_channels}, kernel_size={kernel_size}, stride={stride}, '\n",
        "             'droprate_init={droprate_init}, temperature={temperature}, prior_prec={prior_prec}, '\n",
        "             'lamba={lamba}, local_rep={local_rep}')\n",
        "        if self.padding != (0,) * len(self.padding):\n",
        "            s += ', padding={padding}'\n",
        "        if self.dilation != (1,) * len(self.dilation):\n",
        "            s += ', dilation={dilation}'\n",
        "        if self.output_padding != (0,) * len(self.output_padding):\n",
        "            s += ', output_padding={output_padding}'\n",
        "        if self.groups != 1:\n",
        "            s += ', groups={groups}'\n",
        "        if not self.use_bias:\n",
        "            s += ', bias=False'\n",
        "        s += ')'\n",
        "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jyVxLH0h5HEQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mask_network(network, layers_to_mask, threshold=0.002, linear_masking=None, random_init=False, bias=True, masks=None):\n",
        "    \"\"\"\"\n",
        "    replaces linear layers with masked linear layers\n",
        "    replaces conv layers with masked conv layers\n",
        "    network is the initial sequential container\n",
        "    layers is a list of layers to mask\n",
        "    random init is a logical indicating whether to preserve the initial weights or to modify them\n",
        "    \"\"\"\n",
        "\n",
        "    network.masked_layers=[]\n",
        "    ii = -1\n",
        "    for layer, (name, module) in enumerate(network._modules.items()):\n",
        "        \n",
        "            ii += 1\n",
        "            \n",
        "            layer_mask = None\n",
        "            if masks is not None:\n",
        "                if name in masks:\n",
        "                    layer_mask = masks.get(name)      \n",
        "            if isinstance(module, torch.nn.Linear) and linear_masking is None:\n",
        "                masked_layer = MaskedLinear(layer.in_features, layer.out_features, bias=bias,threshold=threshold,mask=layer_mask)\n",
        "            elif isinstance(module, torch.nn.Linear) and linear_masking =='L0':\n",
        "                _, layer = list(network._modules.items())[ii]\n",
        "                masked_layer = LinearL0(layer.in_features, layer.out_features, bias=bias, lamba=0.1/640)\n",
        "                network.masked_layers.append(masked_layer)\n",
        "            elif isinstance(module, torch.nn.Conv2d):\n",
        "                _, layer = list(network._modules.items())[ii]\n",
        "                masked_layer = L0Conv2d(layer.in_channels, layer.out_channels, layer.kernel_size, layer.stride, layer.padding, layer.dilation, layer.groups, bias=bias, \n",
        "                                        droprate_init=0.5, temperature=2./3., weight_decay=1., lamba=0.1/640, local_rep=False)\n",
        "                network.masked_layers.append(masked_layer)\n",
        "            else:\n",
        "                continue\n",
        "            if random_init != True:\n",
        "                masked_layer.weight = copy.deepcopy(layer.weight)\n",
        "                masked_layer.bias = copy.deepcopy(layer.bias)\n",
        "                \n",
        "            network[int(name)] = masked_layer\n",
        "            \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YbnaQGqI6PAy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VGG_L0(tv_vgg.VGG):\n",
        "    def regularization(self):\n",
        "        regularization = 0.\n",
        "        for layer in self.layers:\n",
        "            regularization += - (1. / self.N) * layer.regularization()\n",
        "#         if torch.cuda.is_available():\n",
        "#             regularization = regularization.cuda()\n",
        "        return regularization\n",
        "    \n",
        "    def regularize(self, N):\n",
        "        regularization = 0.\n",
        "        for layer in self.masked_layers:\n",
        "            regularization += - (1. / N) * layer.regularization()          \n",
        "#         if torch.cuda.is_available():\n",
        "#             regularization = regularization.cuda()\n",
        "        return regularization\n",
        "\n",
        "    def clamp_parameters(self):\n",
        "        for layer in self.masked_layers:\n",
        "            layer.constrain_parameters()\n",
        "    \n",
        "    def get_exp_flops_l0(self):\n",
        "        total_flops = 0\n",
        "        total_l0 = 0\n",
        "#         print(self.masked_layers)\n",
        "        for layer in self.masked_layers:\n",
        "            exp_flops, exp_l0 = layer.count_expected_flops_and_l0()\n",
        "            total_flops += exp_flops\n",
        "            total_l0 += exp_l0\n",
        "        return total_flops, total_l0\n",
        "    \n",
        "    \n",
        "def vgg16_L0(pretrained=False, **kwargs):\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\")\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGG_L0(tv_vgg.make_layers(tv_vgg.cfg['D']), **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(tv_vgg.model_urls['vgg16']))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quImFgeUDM1U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def freeze_layers(model_ft, exclude=[]):\n",
        "#   children = list(model_ft.named_children())\n",
        "    for name,param in model_ft.named_parameters():   \n",
        "        if(name not in exclude):\n",
        "            param.requires_grad = False\n",
        "        \n",
        "def CountNonZeroWeights(model):\n",
        "    \n",
        "    total = 0\n",
        "    zeros = 0\n",
        "    nonzeros = 0\n",
        "    \n",
        "    tol = 1.0e-15\n",
        "    \n",
        "    for name, param in model.named_parameters():\n",
        "        if param is not None:\n",
        "            \n",
        "            param = torch.zeros(3,3)\n",
        "#             print(torch.min(torch.abs(param)))\n",
        "            nonzeros += torch.sum((param != 0.0).int())\n",
        "            zeros += torch.sum((param == 0.0).int())\n",
        "\n",
        "    total = zeros + nonzeros\n",
        "    \n",
        "    print(\"Total number of weights: {}\".format(total))\n",
        "    print(\"Number of non-zero weights: {}\".format(nonzeros))\n",
        "    print(\"Number of zero weights: {}\".format(zeros))\n",
        "    \n",
        "    return total, nonzeros, zeros\n",
        "\n",
        "\n",
        "def set_threshold(model, prop=0.05):\n",
        "    for child in model.named_children():    \n",
        "        for child in child[1].named_children():\n",
        "#       print(child)\n",
        "            if type(child[1]) == MaskedLinear or type(child[1]) == MaskedConv: \n",
        "                child[1].set_threshold(prop=prop)\n",
        "                print(\"layer {}  new threshold {:.4f}\".format(child[0], child[1].threshold))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nclNqiyMDzp9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model_prune(model, dloaders, dataset_sizes, criterion, optimizer, scheduler,prop=0.05, num_epochs=25, device='cuda',pruning='threshold'):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    print(len(dloaders['train']))\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "                data_idx = 0\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                data_idx = 1\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            i=0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dloaders[phase]:               \n",
        "#                 print(\"batch {} phase {}\".format(i, phase))\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    if pruning == 'L0':\n",
        "                        loss = criterion(outputs, labels, model)\n",
        "                    else:\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        model.clamp_parameters()\n",
        "                        exp_flops, exp_l0 = model.get_exp_flops_l0()\n",
        "                i+=1\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                           \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            if epoch % 5 == 0 and phase == 'train': \n",
        "                CountNonZeroWeights(model)\n",
        "                \n",
        "                if pruning == 'threshold':\n",
        "                    set_threshold(model,prop=prop)\n",
        "                elif pruning == 'L0':\n",
        "                    print(\"Expected flops: {} | Expected L0 norm: {}\".format(exp_flops.item(), exp_l0.item()))\n",
        "            \n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YC2Z_YkP68mR",
        "colab_type": "code",
        "outputId": "6caa2539-a91b-484a-c126-436e73fd1516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2490
        }
      },
      "cell_type": "code",
      "source": [
        "def run_normal_training_with_L0_pruning(this_trainset):\n",
        "#     print(this_trainset.__len__())  \n",
        "    _,mytrainset = torch.utils.data.random_split(this_trainset, (49200, 800))\n",
        "    # _,trainset = torch.utils.data.random_split(trainset,(49995,5))\n",
        "#     print(mytrainset.__len__())\n",
        "\n",
        "    mytrain_data, myval_data = torch.utils.data.random_split(mytrainset,(int(0.8*len(mytrainset)),int(0.2*len(mytrainset))))\n",
        "#     print(mytrain_data.__len__(),myval_data.__len__() )\n",
        "\n",
        "    mytrainloader = torch.utils.data.DataLoader(mytrain_data, batch_size=5,\n",
        "                                                shuffle=True, num_workers=0)\n",
        "    myvalloader = torch.utils.data.DataLoader(myval_data, batch_size=5,\n",
        "                                              shuffle=True, num_workers=0)\n",
        "    mydataloaders = {'train': mytrainloader, 'val': myvalloader}\n",
        "    image_datasets = {'train': mytrain_data,'val': myval_data}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}  \n",
        "\n",
        "    model_ft = vgg16_L0(pretrained=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    # ------ Mask classifier layers ------\n",
        "    \n",
        "#     freeze_layers(model_ft.features, exclude=[])\n",
        "    mask_network(model_ft.classifier, [0,3,6], linear_masking=\"L0\")\n",
        "    model_ft.masked_layers = model_ft.classifier.masked_layers\n",
        "    \n",
        "    # ------ Mask conv2D layers ------\n",
        "    \n",
        "    mask_network(model_ft.features, np.arange(0, 30), linear_masking=\"L0\")\n",
        "    model_ft.masked_layers.extend(model_ft.features.masked_layers)\n",
        "\n",
        "    print(\"Number of masked layers: {}\".format(len(model_ft.masked_layers)))\n",
        "\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    def loss_function(outputs, targets, model):\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss += model.regularize(640)\n",
        "        return loss\n",
        "    \n",
        "    # Observe that all parameters are being optimized\n",
        "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "#   [print(p) for p in model_ft.parameters()]\n",
        "#   return\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "    model_ft = train_model_prune(model_ft, mydataloaders, dataset_sizes, loss_function, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=20, pruning=\"L0\")\n",
        "\n",
        "    \n",
        "    \n",
        "transform = transforms.Compose(\n",
        "    [transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=5,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "\n",
        "run_normal_training_with_L0_pruning(trainset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:85: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LinearL0(25088 -> 4096, droprate_init=0.5, lamba=0.00015625, temperature=0.6666666666666666, weight_decay=1.0, local_rep=False)\n",
            "LinearL0(4096 -> 4096, droprate_init=0.5, lamba=0.00015625, temperature=0.6666666666666666, weight_decay=1.0, local_rep=False)\n",
            "LinearL0(4096 -> 1000, droprate_init=0.5, lamba=0.00015625, temperature=0.6666666666666666, weight_decay=1.0, local_rep=False)\n",
            "L0Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "Number of masked layers: 16\n",
            "128\n",
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 52.2036 Acc: 0.0766\n",
            "Total number of weights: 276739174\n",
            "Number of non-zero weights: 276739174\n",
            "Number of zero weights: 2\n",
            "Expected flops: 248126439424.0 | Expected L0 norm: 115086080.0\n",
            "val Loss: 51.8063 Acc: 0.0750\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 51.4088 Acc: 0.0984\n",
            "val Loss: 50.9114 Acc: 0.0750\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 50.4069 Acc: 0.1172\n",
            "val Loss: 49.7403 Acc: 0.0938\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 49.1767 Acc: 0.1125\n",
            "val Loss: 48.4951 Acc: 0.0938\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 48.2310 Acc: 0.1109\n",
            "val Loss: 47.8873 Acc: 0.1125\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 47.8426 Acc: 0.0938\n",
            "Total number of weights: 276739175\n",
            "Number of non-zero weights: 276739175\n",
            "Number of zero weights: 1\n",
            "Expected flops: 248030838784.0 | Expected L0 norm: 115060672.0\n",
            "val Loss: 47.6872 Acc: 0.1063\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 47.6933 Acc: 0.1141\n",
            "val Loss: 47.5875 Acc: 0.0938\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 47.6173 Acc: 0.1109\n",
            "val Loss: 47.5792 Acc: 0.0938\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 47.6042 Acc: 0.1203\n",
            "val Loss: 47.5707 Acc: 0.0938\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 47.6161 Acc: 0.0859\n",
            "val Loss: 47.5639 Acc: 0.0938\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 47.6012 Acc: 0.1156\n",
            "Total number of weights: 276739175\n",
            "Number of non-zero weights: 276739175\n",
            "Number of zero weights: 1\n",
            "Expected flops: 248004214784.0 | Expected L0 norm: 115053424.0\n",
            "val Loss: 47.5555 Acc: 0.0938\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 47.5762 Acc: 0.1109\n",
            "val Loss: 47.5472 Acc: 0.0938\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 47.5781 Acc: 0.0953\n",
            "val Loss: 47.5411 Acc: 0.0938\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 47.5591 Acc: 0.1281\n",
            "val Loss: 47.5335 Acc: 0.0938\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 47.5638 Acc: 0.1094\n",
            "val Loss: 47.5329 Acc: 0.0938\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 47.5705 Acc: 0.0875\n",
            "Total number of weights: 276739175\n",
            "Number of non-zero weights: 276739175\n",
            "Number of zero weights: 1\n",
            "Expected flops: 247998119936.0 | Expected L0 norm: 115051768.0\n",
            "val Loss: 47.5321 Acc: 0.0938\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 47.5581 Acc: 0.1000\n",
            "val Loss: 47.5314 Acc: 0.0938\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 47.5531 Acc: 0.1188\n",
            "val Loss: 47.5307 Acc: 0.0938\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 47.5589 Acc: 0.1063\n",
            "val Loss: 47.5300 Acc: 0.0938\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 47.5574 Acc: 0.1125\n",
            "val Loss: 47.5293 Acc: 0.0938\n",
            "\n",
            "Training complete in 10m 15s\n",
            "Best val Acc: 0.112500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "coPfbZmXE3T0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Driver Routines"
      ]
    },
    {
      "metadata": {
        "id": "DS7qbZ0JE6Fs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Iterative Pruning"
      ]
    },
    {
      "metadata": {
        "id": "E1q-h_KTE-kE",
        "colab_type": "code",
        "outputId": "2f838dbe-e07e-47e8-f512-7dae1bf69042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18307
        }
      },
      "cell_type": "code",
      "source": [
        "# ====== Dataset setup ======\n",
        "\n",
        "percent_data = 5.0\n",
        "percent_val = 20.0\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "# ====== Model setup ======\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# ====== Pruning setup ======\n",
        "\n",
        "N_prune = 0\n",
        "P_prune = 5\n",
        "p = 2\n",
        "prune_settings = UnitPruningSettings(N_prune=N_prune, \n",
        "                                     P_prune=P_prune, \n",
        "                                     p=p, \n",
        "                                     pruning_metric=WEIGHT_NORM)\n",
        "prune_settings.Setup(model)\n",
        "\n",
        "\n",
        "# ====== Begin training ======\n",
        "\n",
        "N_iter_outer = 15\n",
        "N_iter_inner = 10\n",
        "\n",
        "dataset = 'cifar10'\n",
        "\n",
        "# datasets = ('cifar10',\\\n",
        "#             'cifar100',\\\n",
        "#             'mnist',\\\n",
        "#             'fashionmnist',\\\n",
        "#             'kmnist',\\\n",
        "#             'emnist')\n",
        "\n",
        "# Import data\n",
        "dat = DatasetManager(dataset=dataset, \n",
        "                     percent_data=percent_data, \n",
        "                     percent_val=percent_val)\n",
        "\n",
        "dat.ImportDataset(batch_size=batch_size)\n",
        "\n",
        "for ii in range(0, N_iter_outer):\n",
        "    \n",
        "    print(\"\\n------ Outer iteration {}/{} ------\".format(ii+1, N_iter_outer))\n",
        "#     t0 = time.time()\n",
        "\n",
        "    # ------ Prune current model ------\n",
        "        \n",
        "    model = PruneAllConv2DLayers(model, prune_settings)\n",
        "    new_model = copy.deepcopy(model)\n",
        "    model = new_model\n",
        "    prune_settings.PrintPruningStatistics(1)\n",
        "    prune_settings.ResetNormContainers()\n",
        "\n",
        "    # ------ Train current model ------\n",
        "    \n",
        "    # Import data\n",
        "#     dataset = datasets[ii%6]\n",
        "    dataset = 'cifar10'\n",
        "    print(\"Using dataset {}\".format(dataset))\n",
        "    dat = DatasetManager(dataset=dataset, percent_data=percent_data, percent_val=percent_val)\n",
        "    dat.ImportDataset(batch_size=batch_size)\n",
        "    \n",
        "    # Update optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, prune_settings, num_epochs=N_iter_inner)\n",
        "        \n",
        "#     print (\"Pruning took {} s\".format(time.time() - t0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 170483712/170498071 [00:24<00:00, 5666812.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "\n",
            "------ Outer iteration 1/15 ------\n",
            "Total number of filters before pruning: 4224.0\n",
            "Total number of filters after pruning: 4224.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r170500096it [00:40, 5666812.88it/s]                               "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 2.6807 Acc: 0.1205\n",
            "val Loss: 2.1776 Acc: 0.1900\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 2.1622 Acc: 0.2200\n",
            "val Loss: 1.8990 Acc: 0.2420\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.9239 Acc: 0.3000\n",
            "val Loss: 1.7522 Acc: 0.3780\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.8326 Acc: 0.3390\n",
            "val Loss: 1.7636 Acc: 0.3440\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 1.6584 Acc: 0.4125\n",
            "val Loss: 1.6217 Acc: 0.4400\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 1.5409 Acc: 0.4430\n",
            "val Loss: 1.5789 Acc: 0.4140\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 1.5137 Acc: 0.4540\n",
            "val Loss: 1.4315 Acc: 0.5020\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 1.2082 Acc: 0.5795\n",
            "val Loss: 1.2173 Acc: 0.5520\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 1.0815 Acc: 0.5980\n",
            "val Loss: 1.2164 Acc: 0.5440\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 1.0200 Acc: 0.6410\n",
            "val Loss: 1.0802 Acc: 0.6100\n",
            "\n",
            "Training complete in 14m 19s\n",
            "Best val Acc: 0.610000\n",
            "\n",
            "------ Outer iteration 2/15 ------\n",
            "Total number of filters before pruning: 4224.0\n",
            "Total number of filters after pruning: 4020.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.5061 Acc: 0.4705\n",
            "val Loss: 1.5427 Acc: 0.4640\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.4786 Acc: 0.4825\n",
            "val Loss: 1.5898 Acc: 0.4560\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.3876 Acc: 0.5180\n",
            "val Loss: 1.4112 Acc: 0.4980\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.2895 Acc: 0.5500\n",
            "val Loss: 1.2809 Acc: 0.5400\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 1.2749 Acc: 0.5560\n",
            "val Loss: 1.3490 Acc: 0.5080\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 1.2444 Acc: 0.5555\n",
            "val Loss: 1.3128 Acc: 0.5380\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 1.2314 Acc: 0.5745\n",
            "val Loss: 1.2274 Acc: 0.5680\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.8995 Acc: 0.6810\n",
            "val Loss: 1.0457 Acc: 0.6220\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.8213 Acc: 0.7145\n",
            "val Loss: 0.9897 Acc: 0.6560\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.7558 Acc: 0.7245\n",
            "val Loss: 1.0124 Acc: 0.6520\n",
            "\n",
            "Training complete in 14m 2s\n",
            "Best val Acc: 0.656000\n",
            "\n",
            "------ Outer iteration 3/15 ------\n",
            "Total number of filters before pruning: 4020.0\n",
            "Total number of filters after pruning: 3822.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.3110 Acc: 0.5545\n",
            "val Loss: 1.1204 Acc: 0.6220\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.2185 Acc: 0.5760\n",
            "val Loss: 1.1663 Acc: 0.6140\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.1557 Acc: 0.5985\n",
            "val Loss: 1.2528 Acc: 0.5660\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.1299 Acc: 0.6005\n",
            "val Loss: 1.4524 Acc: 0.5100\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 1.1041 Acc: 0.6145\n",
            "val Loss: 1.2187 Acc: 0.5900\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 1.0784 Acc: 0.6230\n",
            "val Loss: 1.1046 Acc: 0.6320\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 1.0698 Acc: 0.6475\n",
            "val Loss: 1.1601 Acc: 0.5840\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.8012 Acc: 0.7190\n",
            "val Loss: 0.9878 Acc: 0.6360\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.7040 Acc: 0.7570\n",
            "val Loss: 0.9738 Acc: 0.6480\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.6510 Acc: 0.7795\n",
            "val Loss: 0.9929 Acc: 0.6820\n",
            "\n",
            "Training complete in 13m 12s\n",
            "Best val Acc: 0.682000\n",
            "\n",
            "------ Outer iteration 4/15 ------\n",
            "Total number of filters before pruning: 3822.0\n",
            "Total number of filters after pruning: 3637.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.2932 Acc: 0.5470\n",
            "val Loss: 1.1922 Acc: 0.5860\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.1753 Acc: 0.5805\n",
            "val Loss: 1.2956 Acc: 0.5200\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.1573 Acc: 0.5850\n",
            "val Loss: 1.1244 Acc: 0.6040\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.0838 Acc: 0.6180\n",
            "val Loss: 1.0209 Acc: 0.6400\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 1.0373 Acc: 0.6360\n",
            "val Loss: 1.2317 Acc: 0.5920\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 1.0168 Acc: 0.6355\n",
            "val Loss: 1.0743 Acc: 0.6040\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.9649 Acc: 0.6675\n",
            "val Loss: 1.3928 Acc: 0.5760\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.7672 Acc: 0.7290\n",
            "val Loss: 0.8651 Acc: 0.6920\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.7172 Acc: 0.7545\n",
            "val Loss: 0.8775 Acc: 0.7080\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.6718 Acc: 0.7515\n",
            "val Loss: 0.9127 Acc: 0.6900\n",
            "\n",
            "Training complete in 12m 11s\n",
            "Best val Acc: 0.708000\n",
            "\n",
            "------ Outer iteration 5/15 ------\n",
            "Total number of filters before pruning: 3637.0\n",
            "Total number of filters after pruning: 3458.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.1560 Acc: 0.5985\n",
            "val Loss: 1.2861 Acc: 0.5480\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.1322 Acc: 0.6025\n",
            "val Loss: 1.2186 Acc: 0.5560\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.0591 Acc: 0.6260\n",
            "val Loss: 1.2222 Acc: 0.5700\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.0371 Acc: 0.6325\n",
            "val Loss: 1.0121 Acc: 0.6480\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 1.0039 Acc: 0.6440\n",
            "val Loss: 1.1757 Acc: 0.6160\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.9636 Acc: 0.6575\n",
            "val Loss: 1.1157 Acc: 0.6180\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.8985 Acc: 0.6960\n",
            "val Loss: 1.0411 Acc: 0.6320\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.7483 Acc: 0.7445\n",
            "val Loss: 0.9355 Acc: 0.6940\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.7151 Acc: 0.7495\n",
            "val Loss: 0.8378 Acc: 0.7080\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.6493 Acc: 0.7665\n",
            "val Loss: 0.9161 Acc: 0.7060\n",
            "\n",
            "Training complete in 11m 46s\n",
            "Best val Acc: 0.708000\n",
            "\n",
            "------ Outer iteration 6/15 ------\n",
            "Total number of filters before pruning: 3458.0\n",
            "Total number of filters after pruning: 3294.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.1796 Acc: 0.5860\n",
            "val Loss: 0.9570 Acc: 0.6580\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.1077 Acc: 0.6195\n",
            "val Loss: 1.0263 Acc: 0.6360\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.0572 Acc: 0.6350\n",
            "val Loss: 1.0925 Acc: 0.6180\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.0326 Acc: 0.6375\n",
            "val Loss: 0.9674 Acc: 0.6520\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 1.0219 Acc: 0.6435\n",
            "val Loss: 1.1001 Acc: 0.6000\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 1.0200 Acc: 0.6530\n",
            "val Loss: 0.9792 Acc: 0.6540\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.9558 Acc: 0.6670\n",
            "val Loss: 0.9318 Acc: 0.7040\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.7681 Acc: 0.7300\n",
            "val Loss: 0.9119 Acc: 0.6960\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.6996 Acc: 0.7575\n",
            "val Loss: 0.8032 Acc: 0.7120\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.6434 Acc: 0.7790\n",
            "val Loss: 0.7749 Acc: 0.7260\n",
            "\n",
            "Training complete in 11m 9s\n",
            "Best val Acc: 0.726000\n",
            "\n",
            "------ Outer iteration 7/15 ------\n",
            "Total number of filters before pruning: 3294.0\n",
            "Total number of filters after pruning: 3136.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.1345 Acc: 0.6080\n",
            "val Loss: 1.3302 Acc: 0.5600\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.0629 Acc: 0.6335\n",
            "val Loss: 1.0932 Acc: 0.6220\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.0132 Acc: 0.6470\n",
            "val Loss: 1.0988 Acc: 0.6160\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.9951 Acc: 0.6570\n",
            "val Loss: 1.1615 Acc: 0.5800\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.9491 Acc: 0.6645\n",
            "val Loss: 1.0357 Acc: 0.6260\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.9432 Acc: 0.6710\n",
            "val Loss: 1.0960 Acc: 0.6100\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.9120 Acc: 0.6955\n",
            "val Loss: 0.9512 Acc: 0.6660\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.7316 Acc: 0.7530\n",
            "val Loss: 0.8718 Acc: 0.7000\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.7068 Acc: 0.7505\n",
            "val Loss: 0.8377 Acc: 0.7020\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.6360 Acc: 0.7790\n",
            "val Loss: 0.8645 Acc: 0.6840\n",
            "\n",
            "Training complete in 10m 12s\n",
            "Best val Acc: 0.702000\n",
            "\n",
            "------ Outer iteration 8/15 ------\n",
            "Total number of filters before pruning: 3136.0\n",
            "Total number of filters after pruning: 2989.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.1031 Acc: 0.6190\n",
            "val Loss: 1.0665 Acc: 0.6080\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.0583 Acc: 0.6390\n",
            "val Loss: 0.9863 Acc: 0.6620\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.0471 Acc: 0.6400\n",
            "val Loss: 0.9990 Acc: 0.6480\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.0172 Acc: 0.6460\n",
            "val Loss: 0.9420 Acc: 0.6580\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.9376 Acc: 0.6630\n",
            "val Loss: 1.0790 Acc: 0.6180\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.9151 Acc: 0.6815\n",
            "val Loss: 1.0920 Acc: 0.6280\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.9085 Acc: 0.6870\n",
            "val Loss: 1.0719 Acc: 0.6240\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.7285 Acc: 0.7455\n",
            "val Loss: 0.8834 Acc: 0.6780\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.6863 Acc: 0.7670\n",
            "val Loss: 0.7898 Acc: 0.7220\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.6239 Acc: 0.7780\n",
            "val Loss: 0.8424 Acc: 0.7000\n",
            "\n",
            "Training complete in 9m 46s\n",
            "Best val Acc: 0.722000\n",
            "\n",
            "------ Outer iteration 9/15 ------\n",
            "Total number of filters before pruning: 2989.0\n",
            "Total number of filters after pruning: 2842.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.1102 Acc: 0.6075\n",
            "val Loss: 0.9935 Acc: 0.6380\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.0589 Acc: 0.6205\n",
            "val Loss: 1.0055 Acc: 0.6480\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.9723 Acc: 0.6535\n",
            "val Loss: 0.9540 Acc: 0.6780\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.9669 Acc: 0.6660\n",
            "val Loss: 1.0296 Acc: 0.6280\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.9230 Acc: 0.6780\n",
            "val Loss: 1.0217 Acc: 0.6440\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.9300 Acc: 0.6805\n",
            "val Loss: 0.9374 Acc: 0.6760\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.8668 Acc: 0.7070\n",
            "val Loss: 1.0830 Acc: 0.6300\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.7275 Acc: 0.7515\n",
            "val Loss: 0.7923 Acc: 0.7080\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.6424 Acc: 0.7775\n",
            "val Loss: 0.8398 Acc: 0.7140\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.6604 Acc: 0.7755\n",
            "val Loss: 0.8331 Acc: 0.6900\n",
            "\n",
            "Training complete in 9m 15s\n",
            "Best val Acc: 0.714000\n",
            "\n",
            "------ Outer iteration 10/15 ------\n",
            "Total number of filters before pruning: 2842.0\n",
            "Total number of filters after pruning: 2704.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.0784 Acc: 0.6180\n",
            "val Loss: 1.0727 Acc: 0.6020\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.0144 Acc: 0.6460\n",
            "val Loss: 0.9955 Acc: 0.6520\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.9932 Acc: 0.6570\n",
            "val Loss: 0.9597 Acc: 0.6560\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.9706 Acc: 0.6715\n",
            "val Loss: 0.9315 Acc: 0.6720\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.9443 Acc: 0.6730\n",
            "val Loss: 1.2509 Acc: 0.5880\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.8826 Acc: 0.7075\n",
            "val Loss: 1.0266 Acc: 0.6520\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.9367 Acc: 0.6725\n",
            "val Loss: 1.0743 Acc: 0.6360\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.7634 Acc: 0.7385\n",
            "val Loss: 0.9291 Acc: 0.6760\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.6867 Acc: 0.7680\n",
            "val Loss: 0.8615 Acc: 0.7160\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.6386 Acc: 0.7750\n",
            "val Loss: 0.7983 Acc: 0.7320\n",
            "\n",
            "Training complete in 8m 55s\n",
            "Best val Acc: 0.732000\n",
            "\n",
            "------ Outer iteration 11/15 ------\n",
            "Total number of filters before pruning: 2704.0\n",
            "Total number of filters after pruning: 2572.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.0920 Acc: 0.6190\n",
            "val Loss: 0.9239 Acc: 0.6840\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 0.9901 Acc: 0.6645\n",
            "val Loss: 0.8722 Acc: 0.6860\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.9576 Acc: 0.6700\n",
            "val Loss: 0.9233 Acc: 0.6760\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.8768 Acc: 0.6830\n",
            "val Loss: 0.9590 Acc: 0.6740\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.9372 Acc: 0.6770\n",
            "val Loss: 0.9371 Acc: 0.6740\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.9021 Acc: 0.6860\n",
            "val Loss: 0.9282 Acc: 0.6780\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.8753 Acc: 0.7070\n",
            "val Loss: 0.9652 Acc: 0.6620\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.7221 Acc: 0.7460\n",
            "val Loss: 0.8254 Acc: 0.7080\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.6821 Acc: 0.7660\n",
            "val Loss: 0.7686 Acc: 0.7260\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.6754 Acc: 0.7590\n",
            "val Loss: 0.7814 Acc: 0.7120\n",
            "\n",
            "Training complete in 8m 7s\n",
            "Best val Acc: 0.726000\n",
            "\n",
            "------ Outer iteration 12/15 ------\n",
            "Total number of filters before pruning: 2572.0\n",
            "Total number of filters after pruning: 2449.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.0318 Acc: 0.6450\n",
            "val Loss: 0.9900 Acc: 0.6360\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.0050 Acc: 0.6420\n",
            "val Loss: 0.9505 Acc: 0.6420\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.9602 Acc: 0.6690\n",
            "val Loss: 1.0281 Acc: 0.6360\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.9216 Acc: 0.6755\n",
            "val Loss: 1.0179 Acc: 0.6340\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.9082 Acc: 0.6740\n",
            "val Loss: 1.0651 Acc: 0.6180\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.8734 Acc: 0.7025\n",
            "val Loss: 0.9633 Acc: 0.6740\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.8938 Acc: 0.6820\n",
            "val Loss: 1.0517 Acc: 0.6440\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.6617 Acc: 0.7660\n",
            "val Loss: 0.8335 Acc: 0.7000\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.6570 Acc: 0.7660\n",
            "val Loss: 0.7996 Acc: 0.7400\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.5723 Acc: 0.7945\n",
            "val Loss: 0.8769 Acc: 0.7060\n",
            "\n",
            "Training complete in 7m 45s\n",
            "Best val Acc: 0.740000\n",
            "\n",
            "------ Outer iteration 13/15 ------\n",
            "Total number of filters before pruning: 2449.0\n",
            "Total number of filters after pruning: 2334.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.0164 Acc: 0.6480\n",
            "val Loss: 0.9708 Acc: 0.6640\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 0.9263 Acc: 0.6725\n",
            "val Loss: 0.9857 Acc: 0.6680\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.9011 Acc: 0.6845\n",
            "val Loss: 1.1183 Acc: 0.6280\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.8888 Acc: 0.6985\n",
            "val Loss: 0.9152 Acc: 0.6800\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.8535 Acc: 0.7045\n",
            "val Loss: 0.8859 Acc: 0.6760\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.7762 Acc: 0.7320\n",
            "val Loss: 1.1340 Acc: 0.6580\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.8295 Acc: 0.7110\n",
            "val Loss: 1.0151 Acc: 0.6520\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.6766 Acc: 0.7730\n",
            "val Loss: 0.8095 Acc: 0.7080\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.6271 Acc: 0.7765\n",
            "val Loss: 0.7895 Acc: 0.7300\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.6094 Acc: 0.7895\n",
            "val Loss: 0.7558 Acc: 0.7580\n",
            "\n",
            "Training complete in 7m 30s\n",
            "Best val Acc: 0.758000\n",
            "\n",
            "------ Outer iteration 14/15 ------\n",
            "Total number of filters before pruning: 2334.0\n",
            "Total number of filters after pruning: 2221.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.0667 Acc: 0.6290\n",
            "val Loss: 0.8838 Acc: 0.6980\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 0.9452 Acc: 0.6510\n",
            "val Loss: 0.9885 Acc: 0.6700\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.9533 Acc: 0.6715\n",
            "val Loss: 0.8396 Acc: 0.6980\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.9085 Acc: 0.6815\n",
            "val Loss: 0.9132 Acc: 0.6740\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.9375 Acc: 0.6630\n",
            "val Loss: 0.9987 Acc: 0.6480\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.9123 Acc: 0.6800\n",
            "val Loss: 0.8547 Acc: 0.6980\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.8690 Acc: 0.7010\n",
            "val Loss: 0.9280 Acc: 0.6680\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.7269 Acc: 0.7425\n",
            "val Loss: 0.7555 Acc: 0.7440\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.6382 Acc: 0.7830\n",
            "val Loss: 0.7430 Acc: 0.7080\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.6364 Acc: 0.7690\n",
            "val Loss: 0.7690 Acc: 0.7160\n",
            "\n",
            "Training complete in 7m 10s\n",
            "Best val Acc: 0.744000\n",
            "\n",
            "------ Outer iteration 15/15 ------\n",
            "Total number of filters before pruning: 2221.0\n",
            "Total number of filters after pruning: 2117.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.0082 Acc: 0.6520\n",
            "val Loss: 0.9115 Acc: 0.6800\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 0.9617 Acc: 0.6615\n",
            "val Loss: 1.0122 Acc: 0.6340\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.9503 Acc: 0.6625\n",
            "val Loss: 0.9677 Acc: 0.6800\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.9071 Acc: 0.6870\n",
            "val Loss: 0.8881 Acc: 0.6760\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.9305 Acc: 0.6725\n",
            "val Loss: 0.9852 Acc: 0.6500\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.8797 Acc: 0.6890\n",
            "val Loss: 0.9760 Acc: 0.6480\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.8606 Acc: 0.7020\n",
            "val Loss: 0.8996 Acc: 0.6820\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.7022 Acc: 0.7530\n",
            "val Loss: 0.7619 Acc: 0.7440\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.6513 Acc: 0.7665\n",
            "val Loss: 0.8217 Acc: 0.7180\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.6580 Acc: 0.7650\n",
            "val Loss: 0.8413 Acc: 0.7100\n",
            "\n",
            "Training complete in 6m 44s\n",
            "Best val Acc: 0.744000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v7ORnILPDbVu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ]
    },
    {
      "metadata": {
        "id": "Icv-APyJDdB5",
        "colab_type": "code",
        "outputId": "2be6337c-92f0-4f20-aa4c-02060ae183ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        }
      },
      "cell_type": "code",
      "source": [
        "# PlotResults(prune_settings, \"pPruneWeightNoNorm5_pre_out_7_in_7_cifar10nore_5percent\")\n",
        "# PlotResults(prune_settings, \"baseline_pre_out_7_in_7_cifar10nore_5percent\")\n",
        "PlotResults(prune_settings, \"pPruneWeightNoNorm5_pre_out_15_in_10_cifar10_5percent\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njuM4BVwUHMdxnAIuCo7jOE6B/wcnur67O9/mMAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/2iWKyV4XjMOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "6wxXo2KqYnxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test"
      ]
    },
    {
      "metadata": {
        "id": "Nl5Oc1wTYpNe",
        "colab_type": "code",
        "outputId": "69f015f1-7d93-4476-aa17-6ef931a84b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import data\n",
        "dataset = 'cifar10'\n",
        "print(\"Testing on dataset {}\".format(dataset))\n",
        "dat = DatasetManager(dataset=dataset, percent_data=percent_data, percent_val=percent_val)\n",
        "dat.ImportDataset(batch_size=batch_size)\n",
        "dataloader = dat.test_loader\n",
        "dataset_size = dat.testset.__len__()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "running_loss = 0.0\n",
        "running_corrects = 0.0\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "since = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "test_loss = running_loss / dataset_size\n",
        "test_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Testing complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "print('Test Loss: {:.4f} Acc: {:.4f}'.format(test_loss, test_acc))\n",
        "\n",
        "print('Accuracy of the network on test images: %d %%' %(100.0*correct/total))\n",
        "\n",
        "# print images\n",
        "# imshow(tv.utils.make_grid(images))\n",
        "# print('Ground truth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing on dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Testing complete in 0m 4s\n",
            "Test Loss: 1.2101 Acc: 0.6080\n",
            "Accuracy of the network on test images: 60 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}