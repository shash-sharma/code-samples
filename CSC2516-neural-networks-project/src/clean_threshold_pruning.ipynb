{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of clean_pruning",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asalcedo31/CSC2516_project/blob/master/clean_threshold_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zZ1NXrBF79lD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# set up\n"
      ]
    },
    {
      "metadata": {
        "id": "WcnGoj4y0dVu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision as tv\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.modules import Module\n",
        "import torchvision.models.vgg as tv_vgg\n",
        "import time\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import re\n",
        "from collections import OrderedDict\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7JL5QilnAFMj",
        "colab_type": "code",
        "outputId": "9bac0e7f-72d9-4f04-b633-c886e1fbea9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "% cd /content/gdrive/My Drive/meta_pruning"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/meta_pruning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wo7QvDNdZKr7",
        "colab_type": "code",
        "outputId": "06710907-f47f-410b-b1d6-586c4744ca64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import torch \n",
        "from PIL import Image\n",
        "# from torch import torchvision\n",
        "from keras.datasets import cifar100\n",
        "import sklearn as sk\n",
        "from sklearn import model_selection\n",
        "\n",
        "class CIFAR100_by_label(Dataset):\n",
        "  def __init__(self,x_data, y_data, transform=None):\n",
        "    self.x = x_data\n",
        "    self.y = y_data\n",
        "    self.transform=transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return(len(self.y))\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    sample_x = self.x[idx,:,:,:]\n",
        "    sample_y = self.y[idx]\n",
        "    \n",
        "    sample_x = Image.fromarray(sample_x,'RGB')  \n",
        "    if(self.transform):      \n",
        "      sample_x = self.transform(sample_x)\n",
        "      sample_y = sample_y\n",
        "    return (sample_x,sample_y)\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fv3fUAPdNeES",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DatasetManager:\n",
        "    \n",
        "    def __init__(self, dataset='cifar10', percent_data=10.0, percent_val=20.0):\n",
        "        \n",
        "        # 'dataset' can be 'cifar10', 'cifar100', 'mnist', 'fashionmnist', 'kmnist', 'emnist', 'stl10', 'svhn'.\n",
        "        # 'percent_data' is the percentage of the full training set to be used.\n",
        "        # 'percent_val' is the percentage of the *loaded* training set to be used as validation data.\n",
        "        self.dataset = dataset\n",
        "        self.data_path = 'data/{}'.format(dataset)\n",
        "        self.percent_data = percent_data\n",
        "        self.percent_val = percent_val\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "            \n",
        "        elif self.dataset == 'cifar10' or\\\n",
        "             self.dataset == 'cifar100' or\\\n",
        "             self.dataset == 'stl10' or\\\n",
        "             self.dataset == 'svhn':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "        \n",
        "        elif self.dataset == 'mnist' or\\\n",
        "             self.dataset == 'fashionmnist' or\\\n",
        "             self.dataset == 'kmnist' or\\\n",
        "             self.dataset == 'emnist':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "        return\n",
        "    \n",
        "    \n",
        "    def ImportDataset(self, batch_size=5):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "        \n",
        "            self.trainset = tv.datasets.ImageFolder(root=os.path.join(self.data_path, \"train\"),\n",
        "                             transform=self.transform)\n",
        "            self.valset = tv.datasets.ImageFolder(os.path.join(self.data_path, \"val\"),\n",
        "                             transform=self.transform)\n",
        "        \n",
        "            self.testset = None\n",
        "        # todo\n",
        "        \n",
        "        elif self.dataset == 'cifar10':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR10(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR10(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "        \n",
        "        elif self.dataset == 'cifar100':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR100(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR100(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "             \n",
        "        elif self.dataset == 'mnist':\n",
        "\n",
        "            self.trainset = tv.datasets.MNIST(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.MNIST(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'fashionmnist':\n",
        "\n",
        "            self.trainset = tv.datasets.FashionMNIST(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.FashionMNIST(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'kmnist':\n",
        "\n",
        "            self.trainset = tv.datasets.KMNIST(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.KMNIST(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'emnist':\n",
        "\n",
        "            self.trainset = tv.datasets.EMNIST(root=self.data_path, split='balanced', train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.EMNIST(root=self.data_path, split='balanced', train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'stl10':\n",
        "\n",
        "            self.trainset = tv.datasets.STL10(root=self.data_path, split='train',\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.STL10(root=self.data_path, split='test',\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'svhn':\n",
        "\n",
        "            self.trainset = tv.datasets.SVHN(root=self.data_path, split='train',\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.SVHN(root=self.data_path, split='test',\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        if self.percent_data < 100:\n",
        "          self.SplitData();\n",
        "        self.GenerateLoaders();\n",
        "                \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def SplitData(self):\n",
        "        \n",
        "        len_full = self.trainset.__len__()\n",
        "        len_train = int(np.round(len_full*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.trainset = torch.utils.data.random_split(self.trainset, (len_full-len_train, len_train))\n",
        "        \n",
        "        len_val = int(np.round(len_train*self.percent_val/100.0))\n",
        "        len_train = len_train - len_val\n",
        "        \n",
        "        self.valset, self.trainset = torch.utils.data.random_split(self.trainset, (len_val, len_train))\n",
        "        \n",
        "        print('\\nFull training set size: {}'.format(len_full))        \n",
        "        print('\\nActive training set size: {}'.format(len_train))\n",
        "        print('Active validation set size: {}'.format(len_val))\n",
        "        \n",
        "        if self.testset is not None:\n",
        "          len_full_test = self.testset.__len__()\n",
        "          len_test = int(np.round(len_full_test*self.percent_data/100.0))\n",
        "        \n",
        "          _, self.testset = torch.utils.data.random_split(self.testset, (len_full_test-len_test, len_test)) \n",
        "          print('Full test set size: {}'.format(len_full_test))\n",
        "          print('Active test set size: {}\\n'.format(len_test))\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def GenerateLoaders(self):\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.val_loader = torch.utils.data.DataLoader(self.valset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        if self.testset is not None:\n",
        "          self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)          \n",
        "            \n",
        "        return\n",
        "    \n",
        "    def DataSamples(self,num_samples=800):\n",
        "      shuffled_train = torch.utils.data.RandomSampler(self.trainset)\n",
        "      self.train_sample_list = list(torch.utils.data.BatchSampler(shuffled_train,num_samples,False))\n",
        "      \n",
        "    def GetDataSample(self):      \n",
        "      for i in range(len(self.train_sample_list)):\n",
        "        yield self.train_sample_list[i]\n",
        "      \n",
        "    def GetDataSampleLoader(self,num_samples=800):\n",
        "        \n",
        "        DS = self.GetDataSample()\n",
        "        train_sample = [trainset[j] for j in DS] \n",
        "\n",
        "        train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "\n",
        "        self.train_loader = torch.utils.data.DataLoader(train_data, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.val_loader = torch.utils.data.DataLoader(val_data, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fmjGlo3V0jCd",
        "colab_type": "code",
        "outputId": "173f4676-5967-47be-a604-514f164ffecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=5,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "# _,trainset = torch.utils.data.random_split(trainset,(49200,800))\n",
        "# _,trainset = torch.utils.data.random_split(trainset,(49995,5))\n",
        "# print(trainset.__len__())\n",
        "\n",
        "# train_data, val_data = torch.utils.data.random_split(trainset,(int(0.8*len(trainset)),int(0.2*len(trainset))))\n",
        "# print(train_data.__len__(),val_data.__len__() )\n",
        "\n",
        "# trainloader = torch.utils.data.DataLoader(train_data, batch_size=5,\n",
        "#                                           shuffle=True, num_workers=0)\n",
        "# valloader = torch.utils.data.DataLoader(val_data, batch_size=5,\n",
        "#                                           shuffle=True, num_workers=0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "15v78wBX0l32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# image_datasets= {'train': train_data,'val': val_data}\n",
        "# dataloaders = {'train': trainloader, 'val': valloader}\n",
        "\n",
        "# dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "# class_names = image_datasets['train'].classes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u6pkokkh0mx_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def freeze_layers(model_ft, exclude=[]):\n",
        "#   children = list(model_ft.named_children())\n",
        "  for name,param in model_ft.named_parameters():   \n",
        "    if(name not in  exclude):\n",
        "      param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PENRAKM0pLT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def countNonZeroWeights(model):\n",
        "    nonzeros = 0\n",
        "    weights = 0\n",
        "    for name,param in model.named_parameters():\n",
        "        if param is not None:\n",
        "            nonzeros += torch.sum((param != 0).int()).data[0]\n",
        "            weights += torch.sum(param).data[0]\n",
        "    \n",
        "    return nonzeros, weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dyXlRGiw0raM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def set_threshold(model,stats, prop=0.05):\n",
        "  total_weights = 0\n",
        "  total_nonzero_weights = 0\n",
        "  for child in model.named_children():    \n",
        "    for child in child[1].named_children():\n",
        "#       print(child)\n",
        "      if type(child[1]) == MaskedLinear or type(child[1]) == MaskedConv: \n",
        "        child[1].set_threshold(prop=prop)\n",
        "        unique_weights = torch.unique(child[1].weight*child[1].mask)\n",
        "        mask_size = child[1].mask.reshape(-1).size()[0]\n",
        "        total_nonzero_weights += torch.sum(child[1].mask.view([mask_size]))\n",
        "        total_weights += mask_size    \n",
        "        print(\"layer {}  new threshold {:.4f}\".format(child[0], child[1].threshold)) \n",
        "  \n",
        "  stats.nonzero_weights.append(float(total_nonzero_weights/total_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kyt4hLKqOJbn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def RecordLosses(phase, epoch_loss, epoch_acc, prune_settings):\n",
        "    \n",
        "    # Record losses for later use, plotting etc\n",
        "    if phase == 'train':\n",
        "        prune_settings.epoch_loss.append(epoch_loss)\n",
        "        prune_settings.epoch_acc.append(epoch_acc)\n",
        "    elif phase == 'val':\n",
        "        prune_settings.val_loss.append(epoch_loss)\n",
        "        prune_settings.val_acc.append(epoch_acc)\n",
        "\n",
        "    return prune_settings\n",
        "\n",
        "\n",
        "def PlotResults(prune_settings,name=None):\n",
        "    \n",
        "    # ====== Plot ======\n",
        "\n",
        "    # ------ Loss ------\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(1, len(prune_settings.epoch_loss)+1), \n",
        "             prune_settings.epoch_loss, \n",
        "             color='red', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Epoch loss')\n",
        "    plt.plot(np.arange(1, len(prune_settings.val_loss)+1), \n",
        "             prune_settings.val_loss, \n",
        "             color='blue', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Validation loss')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    if name is not None:\n",
        "      plt.savefig(name+\"_loss.png\")\n",
        "\n",
        "    # ------ Accuracy ------\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(1, len(prune_settings.epoch_acc)+1), \n",
        "             np.asarray(prune_settings.epoch_acc)*100.0, \n",
        "             color='red', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Epoch accuracy')\n",
        "    plt.plot(np.arange(1, len(prune_settings.val_acc)+1), \n",
        "             np.asarray(prune_settings.val_acc)*100.0, \n",
        "             color='blue', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label = 'Validation accuracy')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    if name is not None:\n",
        "      plt.savefig(name+\"_accuracy.png\")\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def SaveResults(prune_settings, model):\n",
        "    \n",
        "    \n",
        "    \n",
        "    return\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ear7nRQY0wwC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model_prune(model, dat, criterion, optimizer, scheduler, tracker, prop=0.01, num_epochs=25, device='cuda',pruning='threshold'):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    print(dat.trainset.__len__())\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "                data_idx = 0\n",
        "                dataloader = dat.train_loader\n",
        "                dataset_size = dat.trainset.__len__()\n",
        "                \n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                data_idx = 1\n",
        "                dataloader = dat.val_loader\n",
        "                dataset_size = dat.valset.__len__()\n",
        "\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            i=0\n",
        "            \n",
        "#             print(dloaders[phase].__iter__().next())\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloader:               \n",
        "#                 print(\"batch {} phase {}\".format(i, phase))\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    if pruning == 'L0':\n",
        "                      loss = criterion(outputs, labels,model)\n",
        "                    else:\n",
        "                      loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        if pruning == 'L0':\n",
        "                          model.clamp_parameters()\n",
        "                          exp_flops, exp_l0 = model.get_exp_flops_l0()\n",
        "                i+=1\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                           \n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_acc = running_corrects.double() / dataset_size\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            if epoch % 5 == 0 and phase == 'train': \n",
        "              if pruning == 'threshold':\n",
        "                set_threshold(model, tracker, prop=prop)\n",
        "              elif pruning == 'L0':\n",
        "                print(exp_flops.item(), exp_l0.item())\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                \n",
        "            tracker = RecordLosses(phase, epoch_loss, epoch_acc, tracker)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "    tracker.outer_iter_time.append(time_elapsed)\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FS6V4Olo00JW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Masked:\n",
        "  def make_mask(self, threshold,mask=None):\n",
        "    if mask is None:\n",
        "      print(\"new mask\",device)\n",
        "      self.mask = torch.ones(self.weight.size(), requires_grad=False).to(device)\n",
        "    else:\n",
        "      self.mask = mask      \n",
        "    self.zeros = torch.zeros(self.weight.size(), requires_grad=False).to(device)\n",
        "    self.threshold = threshold\n",
        "  def set_threshold(self,prop=0.01):\n",
        "    unique_weights = torch.unique(self.weight*self.mask)\n",
        "    mask_size = self.mask.reshape(-1).size()[0]\n",
        "    mask_nonzero = torch.sum(self.mask.view([mask_size]))\n",
        "    mask_total = mask_size\n",
        "    print('nonzero proportion: {:.4f}'.format(mask_nonzero/mask_total))\n",
        "    self.threshold = torch.max(torch.topk(torch.abs(unique_weights),int(prop*unique_weights.size()[0]),largest=False)[0])    \n",
        "  def make_threshold_mask(self):\n",
        "    self.mask = torch.where(torch.abs(self.weight) >= self.threshold,self.mask,self.zeros).to(device)\n",
        "#     self.mask.requires_grad_(requires_grad=False)\n",
        "  def mask_weight(self):\n",
        "    self.weight = torch.nn.Parameter(self.weight*self.mask).to(device) \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Zw6A3138FNi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# L0 pruning\n"
      ]
    },
    {
      "metadata": {
        "id": "QtGNlj9j04hZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MaskedLinear(torch.nn.Linear,Masked):\n",
        "  def __init__(self, in_features, out_features, bias=True, threshold=float(\"-inf\"),mask=None):\n",
        "    super(MaskedLinear, self).__init__(in_features,out_features)\n",
        "    self.make_mask(threshold,mask)\n",
        "  def forward(self, input):\n",
        "    self.make_threshold_mask()\n",
        "    self.mask_weight()\n",
        "#     print(self.mask[125:135,125:135])\n",
        "#     print(self.weight[125:135,125:135])\n",
        "    return F.linear(input, self.weight, self.bias)\n",
        "\n",
        "class MaskedConv(torch.nn.Conv2d,Masked):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, groups, bias=True,threshold=0.0001):\n",
        "    super(MaskedConv,self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "    self.make_mask(threshold)    \n",
        "  def forward(self, input):\n",
        "    self.make_threshold_mask()\n",
        "    self.mask_weight()\n",
        "    return F.conv2d(input, self.weight, self.bias, self.stride,\n",
        "                    self.padding, self.dilation, self.groups)\n",
        "  \n",
        "limit_a, limit_b, epsilon = -.1, 1.1, 1e-6\n",
        "device='cuda'\n",
        "\n",
        "class LinearL0(Module):\n",
        "  \"\"\"Implementation of L0 regularization for the input units of a fully connected layer\"\"\"\n",
        "  def __init__(self, in_features, out_features, bias=True, weight_decay=1., droprate_init=0.5, temperature=2./3.,\n",
        "                 lamba=1., local_rep=False, qz_loga=None, **kwargs):\n",
        "        \"\"\"\n",
        "        :param in_features: Input dimensionality\n",
        "        :param out_features: Output dimensionality\n",
        "        :param bias: Whether we use a bias\n",
        "        :param weight_decay: Strength of the L2 penalty\n",
        "        :param droprate_init: Dropout rate that the L0 gates will be initialize d to\n",
        "        :param temperature: Temperature of the concrete distribution\n",
        "        :param lamba: Strength of the L0 penalty\n",
        "        :param local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
        "        \"\"\"\n",
        "        super(LinearL0, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.prior_prec = weight_decay\n",
        "        self.weight = torch.nn.Parameter(torch.Tensor(in_features, out_features).to(device))\n",
        "#         self.qz_loga = torch.Tensor(in_features).to(device)\n",
        "        self.qz_loga = torch.nn.Parameter(torch.Tensor(in_features).to(device))\n",
        "        self.temperature = temperature\n",
        "        self.droprate_init = droprate_init if droprate_init != 0. else 0.5\n",
        "        self.lamba = lamba\n",
        "        self.use_bias = False\n",
        "        self.local_rep = local_rep\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.Tensor(out_features).to(device))\n",
        "            self.use_bias = True\n",
        "        self.floatTensor = torch.FloatTensor if not torch.cuda.is_available() else torch.cuda.FloatTensor\n",
        "        self.reset_parameters()\n",
        "        if qz_loga is not None:\n",
        "          self.qz_loga = qz_loga\n",
        "\n",
        "  def reset_parameters(self):\n",
        "      torch.nn.init.kaiming_normal(self.weight, mode='fan_out')\n",
        "      self.qz_loga.data.normal_(math.log(1 - self.droprate_init) - math.log(self.droprate_init), 1e-2)\n",
        "\n",
        "      if self.use_bias:\n",
        "          self.bias.data.fill_(0)\n",
        "\n",
        "  def constrain_parameters(self, **kwargs):\n",
        "      self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
        "\n",
        "  def cdf_qz(self, x):\n",
        "      \"\"\"Implements the CDF of the 'stretched' concrete distribution\"\"\"\n",
        "      xn = (x - limit_a) / (limit_b - limit_a)\n",
        "      logits = math.log(xn) - math.log(1 - xn)\n",
        "      return F.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=epsilon, max=1 - epsilon).to(device)\n",
        "\n",
        "  def quantile_concrete(self, x):\n",
        "      \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "      y = F.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature).to(device)\n",
        "      return y * (limit_b - limit_a) + limit_a\n",
        "\n",
        "  def _reg_w(self):\n",
        "      \"\"\"Expected L0 norm under the stochastic gates, takes into account and re-weights also a potential L2 penalty\"\"\"\n",
        "      logpw_col = torch.sum(- (.5 * self.prior_prec * self.weight.pow(2)) - self.lamba, 1).to(device)\n",
        "      logpw = torch.sum((1 - self.cdf_qz(0)) * logpw_col).to(device)\n",
        "      logpb = 0 if not self.use_bias else - torch.sum(.5 * self.prior_prec * self.bias.pow(2)).to(device)\n",
        "      return logpw + logpb\n",
        "\n",
        "  def regularization(self):\n",
        "      return self._reg_w()\n",
        "\n",
        "  def count_expected_flops_and_l0(self):\n",
        "      \"\"\"Measures the expected floating point operations (FLOPs) and the expected L0 norm\"\"\"\n",
        "      # dim_in multiplications and dim_in - 1 additions for each output neuron for the weights\n",
        "      # + the bias addition for each neuron\n",
        "      # total_flops = (2 * in_features - 1) * out_features + out_features\n",
        "      ppos = torch.sum(1 - self.cdf_qz(0))\n",
        "      expected_flops = (2 * ppos - 1) * self.out_features\n",
        "      expected_l0 = ppos * self.out_features\n",
        "      if self.use_bias:\n",
        "          expected_flops += self.out_features\n",
        "          expected_l0 += self.out_features\n",
        "#       return expected_flops.data[0], expected_l0.data[0]\n",
        "      return expected_flops, expected_l0\n",
        "\n",
        "  def get_eps(self, size):\n",
        "      \"\"\"Uniform random numbers for the concrete distribution\"\"\"\n",
        "      eps = self.floatTensor(size).uniform_(epsilon, 1-epsilon).to(device)\n",
        "      eps = Variable(eps)\n",
        "      return eps\n",
        "\n",
        "  def sample_z(self, batch_size, sample=True):\n",
        "      \"\"\"Sample the hard-concrete gates for training and use a deterministic value for testing\"\"\"\n",
        "      if sample:\n",
        "          eps = self.get_eps(self.floatTensor(batch_size, self.in_features))\n",
        "          z = self.quantile_concrete(eps)\n",
        "          return F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "      else:  # mode\n",
        "          pi = F.sigmoid(self.qz_loga).view(1, self.in_features).expand(batch_size, self.in_features).to(device)\n",
        "          return F.hardtanh(pi * (limit_b - limit_a) + limit_a, min_val=0, max_val=1).to(device)\n",
        "\n",
        "  def sample_weights(self):\n",
        "      z = self.quantile_concrete(self.get_eps(self.floatTensor(self.in_features)))\n",
        "      mask = F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "      return mask.view(self.in_features, 1) * self.weight\n",
        "\n",
        "  def forward(self, input):\n",
        "      if self.local_rep or not self.training:\n",
        "          z = self.sample_z(input.size(0), sample=self.training)\n",
        "          xin = input.mul(z)\n",
        "          output = xin.mm(self.weight)\n",
        "      else:\n",
        "          weights = self.sample_weights()\n",
        "          output = input.mm(weights)\n",
        "      if self.use_bias:\n",
        "          output.add_(self.bias)\n",
        "      return output\n",
        "\n",
        "  def __repr__(self):\n",
        "      s = ('{name}({in_features} -> {out_features}, droprate_init={droprate_init}, '\n",
        "           'lamba={lamba}, temperature={temperature}, weight_decay={prior_prec}, '\n",
        "           'local_rep={local_rep}')\n",
        "      if not self.use_bias:\n",
        "          s += ', bias=False'\n",
        "      s += ')'\n",
        "      return s.format(name=self.__class__.__name__, **self.__dict__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJeDMbDw06i2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mask_network(network,layers_to_mask, n=None, lamba=0.1, threshold=float(\"-inf\"), linear_masking=None,random_init=False, bias=True,masks=None):\n",
        "  \"\"\"\"\n",
        "  replaces linear layers with masked linear layers\n",
        "  network is the initial sequential container\n",
        "  layers is a list of layers to mask\n",
        "  random init is a logical indicating whether to preserve the initial weights or to modify them\n",
        "  \"\"\"\n",
        "  network.masked_layers=[]\n",
        "  for name,layer in network.named_children():   \n",
        "    if int(name) in layers_to_mask:\n",
        "      layer_mask = None\n",
        "      if masks is not None:\n",
        "        if name in masks:\n",
        "          layer_mask = masks.get(name)      \n",
        "      if type(layer)== torch.nn.Linear and linear_masking is None:\n",
        "        masked_layer = MaskedLinear(layer.in_features, layer.out_features, bias=bias,threshold=threshold,mask=layer_mask)\n",
        "      elif type(layer)== torch.nn.Linear and linear_masking =='L0':\n",
        "        masked_layer = LinearL0(layer.in_features, layer.out_features, bias=bias, lamba=0.1/n,qz_loga=layer_mask)\n",
        "#         masked_layer = LinearL0(layer.in_features, layer.out_features, bias=bias)\n",
        "        network.masked_layers.append(masked_layer)\n",
        "      elif type(layer)== torch.nn.Conv2d:\n",
        "        masked_layer = MaskedConv(layer.in_channels, layer.out_channels, layer.kernel_size, layer.stride, layer.padding, layer.dilation,layer.groups, bias=bias, threshold=threshold)\n",
        "      if random_init != True and linear_masking != 'L0':\n",
        "        masked_layer.weight = copy.deepcopy(layer.weight)\n",
        "        masked_layer.bias = copy.deepcopy(layer.bias)\n",
        "      elif random_init != True and linear_masking == 'L0':\n",
        "        weight_copy = torch.transpose(copy.deepcopy(layer.weight),0,1)\n",
        "        masked_layer.weights = torch.nn.Parameter(weight_copy)\n",
        "        \n",
        "      network[int(name)] = masked_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-BTMME3oonFj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class L0_Meta_Objective():\n",
        "  def __init__(self,dat,stat,pretrained=True):\n",
        "#     self.trainloader = torch.utils.data.DataLoader(train_data, batch_size=5,\n",
        "#                                             shuffle=True, num_workers=0)\n",
        "    \n",
        "#     self.valloader = torch.utils.data.DataLoader(val_data, batch_size=5,\n",
        "#                                             shuffle=True, num_workers=0)\n",
        "    \n",
        "#     self.dataloaders = {'train':self.trainloader, 'val': self.valloader}\n",
        "#     self.image_datasets= {'train': train_data,'val': val_data}\n",
        "#     self.dataset_sizes = {x: len(self.image_datasets[x]) for x in ['train', 'val']}\n",
        "    self.dat = dat\n",
        "    self.model_ft = vgg16_L0(pretrained=pretrained)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.model_ft = self.model_ft.to(device)    \n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.stat = stat\n",
        "#     freeze_layers(self.model_ft.features, exclude=[])    \n",
        "    \n",
        "  def add_L0_layers(self,layers_to_prune,initial_masks=None,n=None,lamba=0.1):    \n",
        "    mask_network(self.model_ft.classifier,layers_to_prune,n=n,linear_masking=\"L0\",masks=initial_masks,lamba=lamba)\n",
        "    self.model_ft.masked_layers = self.model_ft.classifier.masked_layers  \n",
        "    return\n",
        "  \n",
        "  def inner_loss_function(self,outputs,targets,model):\n",
        "      loss = self.criterion(outputs,targets)\n",
        "      loss += self.model_ft.regularize(800)\n",
        "      return loss\n",
        "        \n",
        "  def inner_train_loop(self, inner_epochs):    \n",
        "    # Observe that all parameters are being optimized\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    self.optimizer_ft = optim.Adam(self.model_ft.parameters(), lr=0.001)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(self.optimizer_ft, step_size=7, gamma=0.1)\n",
        "    final_mod = train_model_prune(self.model_ft, self.dat, self.inner_loss_function, self.optimizer_ft, exp_lr_scheduler,\n",
        "                                    self.stat, num_epochs=inner_epochs, pruning=\"L0\")\n",
        "    return final_mod\n",
        "    \n",
        "def get_initial_masks(model,layers_to_mask,droprate_init=0.5):\n",
        "    initial_masks = {}    \n",
        "    for name,layer in model.named_children():\n",
        "      if int(name) in layers_to_mask:\n",
        "        qz_loga = torch.nn.Parameter(torch.Tensor(layer.in_features).to(device))\n",
        "        qz_loga.data.normal_(math.log(1 - droprate_init) - math.log(droprate_init), 1e-2)\n",
        "        initial_masks[name] = qz_loga                             \n",
        "    return initial_masks\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VdH3hF8PMqRo",
        "colab_type": "code",
        "outputId": "f0b8572a-3792-44f3-a6e3-b6171f8b7063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1116
        }
      },
      "cell_type": "code",
      "source": [
        "class VGG_L0(tv_vgg.VGG):\n",
        "  def regularization(self):\n",
        "    regularization = 0.\n",
        "    for layer in self.layers:\n",
        "        regularization += - (1. / self.N) * layer.regularization()\n",
        "    if torch.cuda.is_available():\n",
        "        regularization = regularization.cuda()\n",
        "    return regularization\n",
        "  \n",
        "  def regularize(self, N):\n",
        "    regularization = 0.\n",
        "    for layer in self.masked_layers:\n",
        "          regularization += - (1. / N) * layer.regularization()          \n",
        "    if torch.cuda.is_available():\n",
        "        regularization = regularization.cuda()\n",
        "    return regularization\n",
        "    \n",
        "  def clamp_parameters(self):\n",
        "    for layer in self.masked_layers:\n",
        "      layer.constrain_parameters()\n",
        "  \n",
        "  def get_exp_flops_l0(self):\n",
        "    total_flops = 0\n",
        "    total_l0 = 0\n",
        "    for layer in self.masked_layers:\n",
        "      exp_flops, exp_l0 = layer.count_expected_flops_and_l0()\n",
        "      total_flops += exp_flops\n",
        "      total_l0 += exp_l0\n",
        "    return total_flops, total_l0  \n",
        "          \n",
        "\n",
        "def vgg16_L0(pretrained=False, **kwargs):\n",
        "  \"\"\"VGG 16-layer model (configuration \"D\")\n",
        "  Args:\n",
        "      pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "  \"\"\"\n",
        "  if pretrained:\n",
        "      kwargs['init_weights'] = False\n",
        "  model = VGG_L0(tv_vgg.make_layers(tv_vgg.cfg['D']), **kwargs)\n",
        "  if pretrained:\n",
        "      model.load_state_dict(model_zoo.load_url(tv_vgg.model_urls['vgg16']))\n",
        "  return model\n",
        "\n",
        "\n",
        "def run_normal_training_with_L0_pruning(this_trainset):\n",
        "  print(this_trainset.__len__())  \n",
        "  _,mytrainset = torch.utils.data.random_split(this_trainset,(49800,200))\n",
        "  # _,trainset = torch.utils.data.random_split(trainset,(49995,5))\n",
        "  print(mytrainset.__len__())\n",
        "\n",
        "  mytrain_data, myval_data = torch.utils.data.random_split(mytrainset,(int(0.8*len(mytrainset)),int(0.2*len(mytrainset))))\n",
        "  print(mytrain_data.__len__(),myval_data.__len__() )\n",
        "\n",
        "  mytrainloader = torch.utils.data.DataLoader(mytrain_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  myvalloader = torch.utils.data.DataLoader(myval_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  mydataloaders = {'train': mytrainloader, 'val': myvalloader}\n",
        "  image_datasets= {'train': mytrain_data,'val': myval_data}\n",
        "  dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}  \n",
        "\n",
        "  model_ft = vgg16_L0(pretrained=True)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  freeze_layers(model_ft.features, exclude=[])\n",
        "  mask_network(model_ft.classifier,[0,3,6],linear_masking=\"L0\", n=450,random_init=False)\n",
        "  model_ft.masked_layers = model_ft.classifier.masked_layers\n",
        "  \n",
        "  print(model_ft.classifier[0].weights.size())  \n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  def loss_function(outputs,targets, model):\n",
        "    loss = criterion(outputs,targets)\n",
        "    loss += model.regularize(500)\n",
        "    return loss\n",
        "\n",
        "\n",
        "  # Observe that all parameters are being optimized\n",
        "  optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "#   [print(p) for p in model_ft.parameters()]\n",
        "#   return\n",
        "\n",
        "  # Decay LR by a factor of 0.1 every 7 epochs\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "#   MO = L0_Meta_Objective(mytrain_data,myval_data)\n",
        "#   MO.add_L0_layers([0,3,6],n=200)\n",
        "  dat = DatasetManager(dataset='cifar100', \n",
        "                     percent_data=1, \n",
        "                     percent_val=10)\n",
        "  \n",
        "  \n",
        "  dat.ImportDataset(batch_size=5)\n",
        "  stats = SettingsNStats()\n",
        "  model_ft = train_model_prune(model_ft, dat, loss_function, optimizer_ft, exp_lr_scheduler,stats,\n",
        "                       num_epochs=20, pruning=\"L0\")\n",
        "\n",
        "#   model_ft = MO.inner_train_loop(20)\n",
        "  \n",
        "run_normal_training_with_L0_pruning(trainset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "200\n",
            "160 40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:61: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25088, 4096])\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "\n",
            "Active training set size: 450\n",
            "Active validation set size: 50\n",
            "Full test set size: 10000\n",
            "Active test set size: 100\n",
            "\n",
            "450\n",
            "Epoch 0/19\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 67.8180 Acc: 0.0044\n",
            "205670672.0 102844528.0\n",
            "val Loss: 67.6553 Acc: 0.0000\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 67.2572 Acc: 0.0067\n",
            "val Loss: 67.0439 Acc: 0.0400\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 66.4587 Acc: 0.0200\n",
            "val Loss: 66.3273 Acc: 0.0200\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 66.0648 Acc: 0.0089\n",
            "val Loss: 66.0246 Acc: 0.0200\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d083076874b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#   model_ft = MO.inner_train_loop(20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mrun_normal_training_with_L0_pruning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-d083076874b5>\u001b[0m in \u001b[0;36mrun_normal_training_with_L0_pruning\u001b[0;34m(this_trainset)\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSettingsNStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   model_ft = train_model_prune(model_ft, dat, loss_function, optimizer_ft, exp_lr_scheduler,stats,\n\u001b[0;32m--> 101\u001b[0;31m                        num_epochs=20, pruning=\"L0\")\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#   model_ft = MO.inner_train_loop(20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e94a1f232e5a>\u001b[0m in \u001b[0;36mtrain_model_prune\u001b[0;34m(model, dat, criterion, optimizer, scheduler, tracker, prop, num_epochs, device, pruning)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "YaBU663jnTiW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gd_step(cost, params, lrate):\n",
        "    \"\"\"Perform one gradient descent step on the given cost function with learning\n",
        "    rate lrate. Returns a new set of parameters, and (IMPORTANT) does not modify\n",
        "    the input parameters.\"\"\"\n",
        "\n",
        "    cost_grad_fun = torch.autograd.grad(cost)\n",
        "    grads = cost_grad_fun(params)\n",
        "    opt_params = {}\n",
        "    for p in params.keys():\n",
        "        opt_params[p] = params[p] - grads[p]*lrate\n",
        "    return opt_params\n",
        "\n",
        "def set_params(model,inparams):\n",
        "  for name,layer in model.named_children():\n",
        "    for name_p, p in layer.named_parameters():\n",
        "      key = str(name)+'.'+name_p\n",
        "      if key in inparams.keys():\n",
        "#         print(layer._parameters)\n",
        "        layer._parameters[name_p] = inparams[key]\n",
        "#         print(key)\n",
        "#         print(inparams[key])\n",
        "  return\n",
        "\n",
        "\n",
        "\n",
        "#MAML attempt\n",
        "def train_meta_prune_L0(trainset, valset, layers_to_prune,outer_steps=5, inner_steps=5, num_samples=800, inner_lr=0.001,outer_lr=0.001, device='cuda'):  \n",
        "  shuffled_train = torch.utils.data.RandomSampler(trainset)\n",
        "  train_sample_list = list(torch.utils.data.BatchSampler(shuffled_train,num_samples,False))\n",
        "  shuffled_train = [x for x in shuffled_train]\n",
        "  #sample model for dimensions\n",
        "  model_ft = vgg16_L0(pretrained=True)\n",
        "  initial_masks = get_initial_masks(model_ft.classifier, [0])\n",
        "#   outer_optimizer = optim.RMSprop(initial_masks.values(), lr=0.001, momentum=0.9)\n",
        "#   print(initial_masks)\n",
        "  \n",
        "  for i in range(outer_steps):\n",
        "#     for phase in ['train','val']\n",
        "    train_sample = [trainset[j] for j in train_sample_list[i]] \n",
        "    train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "    \n",
        "#     outer_optimizer.zero_grad()\n",
        "    #meta obejctive object contains model and loaders\n",
        "    MO = L0_Meta_Objective(train_data,val_data)\n",
        "        \n",
        "    #make separate inital masks for each layer    \n",
        "    MO.add_L0_layers(layers_to_prune,initial_masks)\n",
        "    print(\"new outer\")\n",
        "    losses_ta\n",
        "    \n",
        "    for i in range(5):\n",
        "      print(\"i \",i)\n",
        "      inputs,labels = MO.trainloader.__iter__().next()\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      preds = MO.model_ft(inputs)\n",
        "      \n",
        "      loss =  F.cross_entropy(preds,labels)+MO.model_ft.regularize(640)\n",
        "      vars = dict(MO.model_ft.classifier.named_parameters())\n",
        "      grad = torch.autograd.grad(loss,vars.values())\n",
        "#       grad = torch.autograd.grad(loss,[MO.model_ft.classifier[0].weights,MO.model_ft.classifier[3].weight, MO.model_ft.classifier[6].weight, MO.model_ft.classifier[0].qz_loga] )\n",
        "      params = {}\n",
        "      \n",
        "      for p in zip(grad,vars.values(),vars.keys()):\n",
        "#         print(p[1]-p[0])\n",
        "        params[p[2]] = p[1] - inner_lr* p[0]\n",
        "      \n",
        "#       print(params['3.weight'])  \n",
        "#       params = dict(p[0]= p[1] - inner_lr* p[0], zip(grad,vars.values(),vars.keys()))\n",
        "      set_params( MO.model_ft.classifier,params)\n",
        "#       loss += self.model_ft.regularize(640)\n",
        "      print('final inner loss {:.4f}'.format(loss))\n",
        "    \n",
        "    inputs,labels = MO.dataloaders['train'].__iter__().next()\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    final_preds = MO.model_ft(inputs)\n",
        "    final_loss =  F.cross_entropy(final_preds,labels)+MO.model_ft.regularize(640)\n",
        "    print('final outer loss {:.4f}'.format(final_loss))\n",
        "    outer_grad = torch.autograd.grad(final_loss,initial_masks.values())\n",
        "    for p in zip(outer_grad,initial_masks.values(),initial_masks.keys()):\n",
        "#           print(p[1]-p[0])\n",
        "        initial_masks[p[2]] = torch.nn.Parameter(p[1] - outer_lr* p[0])\n",
        "\n",
        "# train_meta_prune_L0(trainset,testset,[0], outer_steps=10,inner_lr=0.1, outer_lr=0.01)\n",
        "\n",
        "\n",
        "def set_params_from_t(model,inparams):\n",
        "  for name,layer in model.named_children():\n",
        "    for name_p, p in layer.named_parameters():\n",
        "      key = str(name)+'.'+name_p\n",
        "      if key in inparams.keys():        \n",
        "        p_new = inparams[key].data\n",
        "#         p_new.requires_grad = True        \n",
        "#           model[int(name)].weight = p\n",
        "#         print(model[int(name)]._parameters[name_p])\n",
        "        model[int(name)]._parameters[name_p].data.copy_(p_new)\n",
        "#   print(model[6]._parameters[name_p])\n",
        "  return \n",
        "\n",
        "def meta_prune_reptile(trainset, valset, layers_to_prune,outer_steps=10, only_mask = False, inner_steps=5, num_samples=200, inner_lr=0.001,outer_lr=0.001, device='cuda'):  \n",
        "  shuffled_train = torch.utils.data.RandomSampler(trainset)\n",
        "  train_sample_list = list(torch.utils.data.BatchSampler(shuffled_train,num_samples,False))\n",
        "  \n",
        "  train_sample = train_sample_list[0]\n",
        "  train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "  \n",
        "  MO = L0_Meta_Objective(train_data,val_data)\n",
        "  \n",
        "  #make separate inital masks for each layer    \n",
        "  MO.add_L0_layers(layers_to_prune,n=200)\n",
        "  initial_p = {}\n",
        "  for name, param in OrderedDict(MO.model_ft.classifier.named_parameters()).items():\n",
        "    if re.search('a_logqz',name) or only_mask ==False:\n",
        "      i_p = torch.ones(param.data.size(),requires_grad=False).to(device)\n",
        "      i_p.copy_(param.data)\n",
        "      initial_p[name] = i_p\n",
        "\n",
        "    \n",
        "  for i in range(1,outer_steps):\n",
        "    print(\"new outer \", i)\n",
        "    train_sample = [trainset[j] for j in train_sample_list[i]] \n",
        "    train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "    MO = L0_Meta_Objective(train_data,val_data)\n",
        "    MO.add_L0_layers(layers_to_prune,n=200)\n",
        "    print('initial',initial_p['6.qz_loga'][125:130])\n",
        "#     print(dataloaders['train'].__iter__().next())\n",
        "#     set_params_from_t(MO.model_ft.classifier, initial_p)\n",
        "  \n",
        "#     print('weights',MO.model_ft.classifier[6].weight[125:130,125:130])\n",
        "    new_mod = MO.inner_train_loop(5)\n",
        "#     print('after', MO.model_ft.classifier[6].weight[125:130,125:130])\n",
        "#     print('initial after',initial_p['6.weight'][125:130,125:130])\n",
        "    new_p = OrderedDict(MO.model_ft.classifier.named_parameters())\n",
        "    \n",
        "    for name, p in initial_p.items():\n",
        "      update_p = (p - new_p[name].data)/outer_steps*outer_lr\n",
        "      if name == '6.qz_loga':\n",
        "#         print('p',p[125:130,125:130])\n",
        "        print('update',update_p[125:130])\n",
        "      initial_p[name] = p+update_p.data\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "#   for i in range(outer_steps):\n",
        "# meta_prune_reptile(trainset,testset,[0,3,6], outer_steps=20,inner_lr=0.1, outer_lr=0.1)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A0WkKWn3-X8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# run"
      ]
    },
    {
      "metadata": {
        "id": "gcCzS1xv6nZY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_normal_training_with_pruning(this_trainset):\n",
        "  _,mytrainset = torch.utils.data.random_split(this_trainset,(49200,800))\n",
        "\n",
        "  mytrain_data, myval_data = torch.utils.data.random_split(mytrainset,(int(0.8*len(mytrainset)),int(0.2*len(mytrainset))))\n",
        "  print(mytrain_data.__len__(),myval_data.__len__() )\n",
        "\n",
        "  mytrainloader = torch.utils.data.DataLoader(mytrain_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  myvalloader = torch.utils.data.DataLoader(myval_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  mydataloaders = {'train': mytrainloader, 'val': myvalloader}\n",
        "  image_datasets= {'train': mytrain_data,'val': myval_data}\n",
        "  dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "  model_ft = models.vgg16(pretrained=True)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  freeze_layers(model_ft.features, exclude=[])\n",
        "  mask_network(model_ft.classifier,[0],threshold=0.0001)\n",
        "  set_threshold(model_ft)\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()  \n",
        "     \n",
        "  # Observe that all parameters are being optimized\n",
        "  optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  # Decay LR by a factor of 0.1 every 7 epochs\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "  \n",
        "  \n",
        "  model_ft = train_model_prune(model_ft, mydataloaders,dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=2)\n",
        "  \n",
        "# run_normal_training_with_pruning(trainset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iq2zOzYatjQP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Baseline model"
      ]
    },
    {
      "metadata": {
        "id": "KEqbbmGDvQhp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SettingsNStats:\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        # EITHER N_prune OR P_prune will be used to decide how many filters to prune.\n",
        "        # If one is non-positive, the other is used.\n",
        "        # If neither is non-positive, priority is given to P_prune.\n",
        "        # If both are non-positive, no pruning will happen.\n",
        "        \n",
        "        # Keep track of running epoch loss and validation loss, and corresponding accuracy\n",
        "        self.nonzero_weights = []\n",
        "        self.epoch_loss = []\n",
        "        self.val_loss = []\n",
        "        self.epoch_acc = []\n",
        "        self.val_acc = []\n",
        "        self.outer_iter_time = []\n",
        "        return\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wKpJS6fxg4sN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_basics(model= None, pretrained=True, dataset='cifar10',percent_data=2,percent_val=25, batch_size=5):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  if model is None:\n",
        "    model_ft = models.vgg16(pretrained=pretrained) \n",
        "    \n",
        "  else:\n",
        "    model_ft = model\n",
        "  model_ft = model_ft.to(device)    \n",
        "  \n",
        "\n",
        "  stats = SettingsNStats()\n",
        "\n",
        "  # ====== Dataset setup ======\n",
        "\n",
        "  dat = DatasetManager(dataset=dataset, \n",
        "                     percent_data=percent_data, \n",
        "                     percent_val=percent_val)\n",
        "  \n",
        "  \n",
        "  dat.ImportDataset(batch_size=batch_size)\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Observe that all parameters are being optimized\n",
        "  optimizer = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  # Decay LR by a factor of 0.1 every 7 epochs\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) \n",
        "  return model_ft, stats, dat, optimizer, criterion, exp_lr_scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eW7J6ecMYWYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_baseline_model(model= None, pretrained=True, dataset='cifar10', num_epochs=25, percent_data=2,percent_val=25, batch_size=5):\n",
        "  model_ft, stats, dat, optimizer, criterion, exp_lr_scheduler = get_basics(model=model, pretrained=pretrained, dataset=dataset, percent_data = percent_data,  percent_val = percent_val, batch_size=batch_size)\n",
        "\n",
        "  model_ft = train_model_prune(model_ft, dat, criterion, optimizer, exp_lr_scheduler, stats, num_epochs=num_epochs, pruning=None)\n",
        "  return(model_ft, stats)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1efXG64L-f6p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Baseline from scratch\n",
        "# baseline_model_pretrained, baseline_stats = run_baseline_model(pretrained=False, num_epochs=30)\n",
        "# PlotResults(baseline_stats, name=\"baseline_10\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VnfkypSF-Toz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fine tune the pre-trained model on a larger dataset\n",
        "# fmnist_baseline, fmnist_baseline_stats = run_baseline_model( dataset='fashionmnist', percent_data=2., num_epochs=30)\n",
        "# PlotResults(fmnist_baseline_stats, name=\"fmnist_baseline_\")\n",
        "\n",
        "# baseline_model_T, baseline_stats_pretrained_T = run_baseline_model(model=baseline_model_pretrained, dataset='fashionmnist', percent_data=2., num_epochs=30)\n",
        "# PlotResults(baseline_stats_pretrained_T, name=\"baseline_T\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pdXlcGrydjXt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def threshold_masking(model=None, pretrained=True, dataset='cifar10',prop=0.05, mask_classifier=False, mask_conv_weights=False, num_epochs=25, percent_data=2,percent_val=25, batch_size=5):\n",
        "  model_ft, stats, dat, optimizer, criterion, exp_lr_scheduler = get_basics(model=model, pretrained=pretrained, dataset=dataset, percent_data = percent_data,  percent_val = percent_val, batch_size=batch_size)\n",
        "#   freeze_layers(model_ft.features)   \n",
        "  if mask_classifier ==True:\n",
        "    mask_network(model_ft.classifier,[0,3])\n",
        "    \n",
        "  if mask_conv_weights == True:\n",
        "    mask_network(model_ft.features,[0,2,5,7,10,12,14,17,19,21,24,26,28],threshold=0.0001)\n",
        "  stats = SettingsNStats()\n",
        "  \n",
        "  model_ft = train_model_prune(model_ft, dat, criterion, optimizer, exp_lr_scheduler, stats, prop=prop, num_epochs=num_epochs, pruning='threshold')\n",
        "  return(model_ft,stats)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kUNlPalBeUcR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#classifier single threshold pruning\n",
        "# ct0_model, ct0_stats = threshold_masking(pretrained=False, prop=0.05, num_epochs=30, mask_classifier=True)\n",
        "# PlotResults(ct0_stats, name=\"classifier single_threshold_CF10\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wKtKA_VQu6Nb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#fine tune the pre-trained model on a larger dataset\n",
        "# ct0_model_T, ct0_stats_pretrained_T = threshold_masking(model=ct0_model, dataset='fashionmnist', percent_data=2., prop=0, num_epochs=30, mask_classifier=False)\n",
        "# PlotResults(ct0_stats_pretrained_T, name=\"ct0_T\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OqeN3U0CeDqd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#classifier single threshold pruning\n",
        "# ct1_model, ct1_stats = threshold_masking(pretrained=False, prop=0.05, num_epochs=20, mask_conv_weights=True)\n",
        "# PlotResults(ct1_stats, name=\"conv single_threshold_CF10\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BG5nlGQ3hEQf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fine tune the pre-trained model on a larger dataset\n",
        "# ct1_model_T, ct1_stats_pretrained_T = threshold_masking(model=ct1_model, dataset='fashionmnist', percent_data=2., prop=0, num_epochs=20, mask_classifier=False)\n",
        "# PlotResults(ct1_stats_pretrained_T, name=\"conv single_threshold_CF10\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GShMvVfL0-mQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_masks(model, layers_to_prune):\n",
        "  mask_dict = {}\n",
        "  for layer in layers_to_prune:\n",
        "    mask_dict[str(layer)] = torch.ones(model.weight.size()).to(device)\n",
        "  return  mask_dict\n",
        "  \n",
        "def train_iterative_pruning(model,trainset, outer_steps, layers_to_prune, num_samples=800, device='cuda'):\n",
        "  if model is not None:    \n",
        "    model_ft = models.vgg16(pretrained=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_ft = model_ft.to(device)\n",
        "  \n",
        "  model_ft, stats, dat, optimizer, criterion, exp_lr_scheduler = get_basics(model=model, pretrained=pretrained, dataset=dataset, percent_data = percent_data,  percent_val = percent_val, batch_size=batch_size)\n",
        "  dat.DataSamples(num_samples=800)\n",
        "  mask_dict = initialize_mask(model, layers_to_prune)\n",
        "\n",
        "  for i in range(outer_steps):\n",
        "    train_sample = [trainset[j] for j in train_sample_list[i]] \n",
        "    \n",
        "#     print(len(train_sample))\n",
        "#     _,train_sample = torch.utils.data.random_split(trainset,(49200,800))\n",
        "    train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "    valloader = torch.utils.data.DataLoader(val_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "    \n",
        "    subdataloaders = {'train': trainloader, 'val': valloader}\n",
        "    image_datasets= {'train': train_data,'val': val_data}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    \n",
        "    model_ft = models.vgg16(pretrained=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "    # freeze_layers(model_ft.features, exclude=['28.weight'])\n",
        "    freeze_layers(model_ft.features)   \n",
        "    mask_network(model_ft.classifier,[0],threshold=0.0001,masks=mask_dict)\n",
        "    model_ft = train_model_prune(model_ft, subdataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=10, prop=0.1, pruning='threshold')\n",
        "    mask_dict = {'0':model_ft.classifier[0].mask}\n",
        "#     set_threshold(model_ft)\n",
        "\n",
        "#     cost = meta_objective({'train':trainloader, 'val':valoader}, model, optimizer, inner_epochs)\n",
        "\n",
        "\n",
        "# train_meta_prune(model_ft,trainset,15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LYUr_RTPTAgc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}